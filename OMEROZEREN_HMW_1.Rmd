---
title: "HMW 1- Data 621"
author: "OMER OZEREN"
output:
  word_document:
    toc_depth: '3'
  html_document:
    df_print: paged
    toc_depth: '3'
  rmdformats::readthedown:
    code_folding: hide
    highlight: kate
    toc_depth: 3
always_allow_html: yes
---
```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE, comment=NA, message=FALSE, warning=FALSE)
library(tidyverse)
library(kableExtra)
library(caret)
library(corrplot)
library(gridExtra)
library(Hmisc)
library(DMwR)
```

## INTRODUCTION

```{r read_data, echo=FALSE}
# Read in the training data
training <- read.csv("https://raw.githubusercontent.com/omerozeren/DATA621/master/moneyball-training-data.csv") 
# Read in the evaluation data
evaluation <- read.csv("https://raw.githubusercontent.com/omerozeren/DATA621/master/moneyball-evaluation-data.csv")
```

I have been given a dataset with `r nrow(training)` records summarizing a major league baseball team's season.  All statistics have been adjusted to match the performance of a 162 game season. Each record represents a professional baseball team from the years 1871 to 2006 inclusive. Each record
has the performance of the team for the given year, with all of the statistics adjusted to match the performance of
a 162 game season. The objective is to build a linear regression model to predict the number of wins for a team.

This report covers an attempt to build a model to predict number of wins of a baseball team in a season based on several offensive and deffensive statistics. Resulting model explained about 39% of variability in the target variable and included most of the provided explanatory variables. Some potentially  variables were not included in the data set due to missing values.I used KNN for variable missing values imputtion.

## DATA EXPLORATION

Each record in the data set represents the performance of the team for the given year adjusted to the current length of the season - 162 games. The data set includes 16 variables and the training set includes 2,276 records.
Following Table show variable Statistical DEscriptions :

```{r echo=FALSE, warning=FALSE, message=FALSE}
sumtable = data.frame(Variable = character(),
                   Min = integer(),
                   Median = integer(),
                   Mean = double(),
                   SD = double(),
                   Max = integer(),
                   Num_Zeros = integer(),
                   Num_NaN = integer())
for (i in 2:17) {
  sumtable <- rbind(sumtable, data.frame(Variable = colnames(training)[i],
                                   Min = min(training[,i], na.rm=TRUE),
                                   Median = median(training[,i], na.rm=TRUE),
                                   Mean = round(mean(training[,i], na.rm=TRUE)),
                                   SD = round(sd(training[,i], na.rm=TRUE)),
                                   Max = max(training[,i], na.rm=TRUE),
                                   Num_Zeros = length(which(training[,i]==0)),
                                   Num_NaN = sum(is.na(training[,i])))
                 )
}
colnames(sumtable) <- c("", "Min", "Median", "Mean", "SD", "Max","Num_Zeros", "Num_NaN")
sumtable
```

Some initial observations:  

* The response variable (`TARGET_WINS`) looks to be normally distributed.  This supports the working theory that there are good teams and bad teams.  There are also a lot of average teams.
* There are also quite a few variables with missing values.  I may need to deal with these in order to have the largest data set possible for modeling.
* A couple variables are bimodal (`TEAM_BATTING_HR`, `TEAM_BATTING_SO` `TEAM_PITCHING_HR`).  This may be a challenge as some of them are missing values and that may be a challenge in filling in missing values.
* Some variables are right skewed (`TEAM_BASERUN_CS`, `TEAM_BASERUN_SB`, etc.).  This might support the good team theory.  It may also introduce non-normally distributed residuals in the model. 


#### Correlations Matrix Table

```{r echo=FALSE, warning=FALSE, message=FALSE}
cm <- cor(training, use="pairwise.complete.obs")
cm <- cm[2:17,2:17]
names <- c("Wins", "H", "2B", "3B", "HR", "BB", "SO", "SB", "CS", "HBP", "P-H", "P-HR", "P-BB", "P-SO", "E", "DP")
colnames(cm) <- names; rownames(cm) <- names
round(cm,2)
```

#### Correlations Matrix Plots

Let's take a look at the correlations.  The following is the correlations from the complete cases only:

```{r echo=FALSE, warning=FALSE, message=FALSE}
cm <- cor(training, use="pairwise.complete.obs")
cm <- cm[2:17,2:17]
names <- c("Wins", "H", "2B", "3B", "HR", "BB", "SO", "SB", "CS", "HBP", "P-H", "P-HR", "P-BB", "P-SO", "E", "DP")
colnames(cm) <- names; rownames(cm) <- names
corrplot(cm, method = "color", type = "upper", tl.col = "black", diag = FALSE)
```

Anything over 0.5 or under -0.5 is highlighted in blue. The matrix was created using complete pairwise observations.

A few conclusions: 

- Not surprisingly there is a very strong correlation between home runs batted in and home runs given up by pitching.
- There is a negative correlation between number of triples and home runs. A less powerful team may not have enough power to hit home runs, but they get a lot of triples. 
- THere is a strong positive correlation between number of strikeouts and home runs. More swings of the bat results in more home runs. 

#### Correlations: Endogenous and  Exogenous Variables

Let's take a look at how the Exogenous(Model Inputs) are correlated with the response variable(Endogenous):

```{r echo=FALSE, warning=FALSE, message=FALSE}
training %>%
  gather(variable, value, -TARGET_WINS) %>%
  ggplot(., aes(value, TARGET_WINS)) + 
  geom_point(fill = "indianred4", color="indianred4") + 
  geom_smooth(method = "lm", se = FALSE, color = "black") + 
  facet_wrap(~variable, scales ="free", ncol = 4) +
  labs(x = element_blank(), y = "Wins")
```

### Variable chacteristics
Each variable is presented below with corresponding basic statistics (minimum, median and maximum values, mean and standard deviation, number of records with missing values), boxplot, density plot with highlighted mean value, and scatterplot against outcome variable (`TARGET_WINS`) with best fit line. This information is used to check general validity of data and adjust as necessary. 
 

#### TEAM_BATTING_H: 

This variable represents number of team base hits:

```{r echo=FALSE, warning=FALSE, message=FALSE, fig.width=10, fig.height=6}
sumtable[sumtable[,1]=="TEAM_BATTING_H",2:8]

# Boxplot
ggplot(training, aes(x = 1, y = TEAM_BATTING_H)) + 
  stat_boxplot(geom ='errorbar') + geom_boxplot() + 
  xlab("Boxplot") + ylab("") + theme(axis.text.x=element_blank(), axis.ticks.x=element_blank())

# Density plot
ggplot(training, aes(x = TEAM_BATTING_H)) + 
  geom_histogram(aes(y=..density..), colour="black", fill="white",bins=50) +
  geom_density(alpha=.2, fill="#FF6666") + ylab("") + xlab("Density Plot with Mean") +
  geom_vline(aes(xintercept=mean(TEAM_BATTING_H, na.rm=TRUE)), color="red", linetype="dashed", size=1)

# Scatterplot
ggplot(data=training, aes(x=TEAM_BATTING_H, y=TARGET_WINS)) + 
  geom_point() + geom_smooth(method = "loess") +
  xlab("Scatterplot with Best Fit Line")
```

**Data Overview:** There are no missing values. The range and distribution are reasonable.


#### TEAM_BATTING_2B:

This variable represents number of team doubles:

```{r echo=FALSE, warning=FALSE, message=FALSE, fig.width=10, fig.height=6}
sumtable[sumtable[,1]=="TEAM_BATTING_2B",2:8]
# Boxplot
ggplot(training, aes(x = 1, y = TEAM_BATTING_2B)) + 
  stat_boxplot(geom ='errorbar') + geom_boxplot() + 
  xlab("Boxplot") + ylab("") + theme(axis.text.x=element_blank(), axis.ticks.x=element_blank())
# Density plot
ggplot(training, aes(x = TEAM_BATTING_2B)) + 
  geom_histogram(aes(y=..density..), colour="black", fill="white",bins=50) +
  geom_density(alpha=.2, fill="#FF6666") + ylab("") + xlab("Density Plot with Mean") +
  geom_vline(aes(xintercept=mean(TEAM_BATTING_2B, na.rm=TRUE)), color="red", linetype="dashed", size=1)
# Scatterplot
ggplot(data=training, aes(x=TEAM_BATTING_2B, y=TARGET_WINS)) + 
  geom_point() + geom_smooth(method = "loess") +
  xlab("Scatterplot with Best Fit Line")

```

**Data Overview:** There are no missing values. The range and distribution are reasonable.


#### TEAM_BATTING_3B: 

This variable represents  number of team triples:

```{r echo=FALSE, warning=FALSE, message=FALSE, fig.width=10, fig.height=6}
sumtable[sumtable[,1]=="TEAM_BATTING_3B",2:8]
# Boxplot
ggplot(training, aes(x = 1, y = TEAM_BATTING_3B)) + 
  stat_boxplot(geom ='errorbar') + geom_boxplot() + 
  xlab("Boxplot") + ylab("") + theme(axis.text.x=element_blank(), axis.ticks.x=element_blank())
# Density plot
ggplot(training, aes(x = TEAM_BATTING_3B)) + 
  geom_histogram(aes(y=..density..), colour="black", fill="white",bins=50) +
  geom_density(alpha=.2, fill="#FF6666") + ylab("") + xlab("Density Plot with Mean") +
  geom_vline(aes(xintercept=mean(TEAM_BATTING_3B, na.rm=TRUE)), color="red", linetype="dashed", size=1)
# Scatterplot
ggplot(data=training, aes(x=TEAM_BATTING_3B, y=TARGET_WINS)) + 
  geom_point() + geom_smooth(method = "loess") +
  xlab("Scatterplot with Best Fit Line")

```

**Data Overview:**The range and distribution are reasonable. There are 2 records with zero values which is unrealistic for a team in a season. One record (index 1347) has 12 variables with missing values, including the outcome variable. This record will be deleted from the data set. Second record (index 1494) has 7 missing variables, but it does have some recorded values in all categories - batting, pitching and fielding. Zero value for `TEAM_BATTING_3B` can be replaced with the median (because the distribution is right-skewed, median value will provide more realistic estimate).


#### TEAM_BATTING_HR: 

This variable represents  number of team triples:

```{r echo=FALSE, warning=FALSE, message=FALSE, fig.width=10, fig.height=6}
sumtable[sumtable[,1]=="TEAM_BATTING_HR",2:8]
# Boxplot
ggplot(training, aes(x = 1, y = TEAM_BATTING_HR)) + 
  stat_boxplot(geom ='errorbar') + geom_boxplot() + 
  xlab("Boxplot") + ylab("") + theme(axis.text.x=element_blank(), axis.ticks.x=element_blank())
# Density plot
ggplot(training, aes(x = TEAM_BATTING_HR)) + 
  geom_histogram(aes(y=..density..), colour="black", fill="white",bins=50) +
  geom_density(alpha=.2, fill="#FF6666") + ylab("") + xlab("Density Plot with Mean") +
  geom_vline(aes(xintercept=mean(TEAM_BATTING_HR, na.rm=TRUE)), color="red", linetype="dashed", size=1)
# Scatterplot
ggplot(data=training, aes(x=TEAM_BATTING_HR, y=TARGET_WINS)) + 
  geom_point() + geom_smooth(method = "loess") +
  xlab("Scatterplot with Best Fit Line")

```

**Data Overview:**There are some low values in the data. So zero doesn't seem too unusual here either.


#### TEAM_BATTING_BB: 

This variable represents Number of team walks

```{r echo=FALSE, warning=FALSE, message=FALSE, fig.width=10, fig.height=6}
sumtable[sumtable[,1]=="TEAM_BATTING_BB",2:8]
# Boxplot
ggplot(training, aes(x = 1, y = TEAM_BATTING_BB)) + 
  stat_boxplot(geom ='errorbar') + geom_boxplot() + 
  xlab("Boxplot") + ylab("") + theme(axis.text.x=element_blank(), axis.ticks.x=element_blank())
# Density plot
ggplot(training, aes(x = TEAM_BATTING_BB)) + 
  geom_histogram(aes(y=..density..), colour="black", fill="white",bins=50) +
  geom_density(alpha=.2, fill="#FF6666") + ylab("") + xlab("Density Plot with Mean") +
  geom_vline(aes(xintercept=mean(TEAM_BATTING_BB, na.rm=TRUE)), color="red", linetype="dashed", size=1)
# Scatterplot
ggplot(data=training, aes(x=TEAM_BATTING_BB, y=TARGET_WINS)) + 
  geom_point() + geom_smooth(method = "loess") +
  xlab("Scatterplot with Best Fit Line")
```

**Data Overview:** The range and distribution are reasonable. There is one record (index 1347) that has a zero value. This record was discussed above (under `TEAM_BATTING_3B`) and it will be deleted from the data set.


#### TEAM_BATTING_HBP: 

This variable represents Number of team batters hit by pitch

```{r echo=FALSE, warning=FALSE, message=FALSE, fig.width=10, fig.height=6}
sumtable[sumtable[,1]=="TEAM_BATTING_HBP",2:8]
# Boxplot
ggplot(training, aes(x = 1, y = TEAM_BATTING_HBP)) + 
  stat_boxplot(geom ='errorbar') + geom_boxplot() + 
  xlab("Boxplot") + ylab("") + theme(axis.text.x=element_blank(), axis.ticks.x=element_blank())
# Density plot
ggplot(training, aes(x = TEAM_BATTING_HBP)) + 
  geom_histogram(aes(y=..density..), colour="black", fill="white",bins=50) +
  geom_density(alpha=.2, fill="#FF6666") + ylab("") + xlab("Density Plot with Mean") +
  geom_vline(aes(xintercept=mean(TEAM_BATTING_HBP, na.rm=TRUE)), color="red", linetype="dashed", size=1)
# Scatterplot
ggplot(data=training, aes(x=TEAM_BATTING_HBP, y=TARGET_WINS)) + 
  geom_point() + geom_smooth(method = "loess") +
  xlab("Scatterplot with Best Fit Line")

```

**Data Overview:** There are 2,085 records - 91.6% of data set - that are missing value. Because this variable is missing for majority of records, I wont consider this variable as input for regression model.

#### TEAM_BATTING_SO: 

This variable represents Number of team strikeouts by batters

```{r echo=FALSE, warning=FALSE, message=FALSE, fig.width=10, fig.height=6}
sumtable[sumtable[,1]=="TEAM_BATTING_SO",2:8]
# Boxplot
ggplot(training, aes(x = 1, y = TEAM_BATTING_SO)) + 
  stat_boxplot(geom ='errorbar') + geom_boxplot() + 
  xlab("Boxplot") + ylab("") + theme(axis.text.x=element_blank(), axis.ticks.x=element_blank())
# Density plot
ggplot(training, aes(x = TEAM_BATTING_SO)) + 
  geom_histogram(aes(y=..density..), colour="black", fill="white",bins=50) +
  geom_density(alpha=.2, fill="#FF6666") + ylab("") + xlab("Density Plot with Mean") +
  geom_vline(aes(xintercept=mean(TEAM_BATTING_SO, na.rm=TRUE)), color="red", linetype="dashed", size=1)
# Scatterplot
ggplot(data=training, aes(x=TEAM_BATTING_SO, y=TARGET_WINS)) + 
  geom_point() + geom_smooth(method = "loess") +
  xlab("Scatterplot with Best Fit Line")

```

**Data Overview:** There are 122 records with missing or zero value (as wtih other variables a zero value is unrealistic). These values can be imputed. Similarly to homeruns, the distribution is multimodal, which is interesting enough for additional analysis. Another area of concern is a noticeable left tail. It is highly unlikely to have games without any strikeouts, so anything lower than 162 (average of 1 strikeout per game) is definitely suspect.

#### TEAM_BASERUN_SB: 

This variable represents Number of team stolen bases

```{r echo=FALSE, warning=FALSE, message=FALSE, fig.width=10, fig.height=6}
sumtable[sumtable[,1]=="TEAM_BASERUN_SB",2:8]
# Boxplot
ggplot(training, aes(x = 1, y = TEAM_BASERUN_SB)) + 
  stat_boxplot(geom ='errorbar') + geom_boxplot() + 
  xlab("Boxplot") + ylab("") + theme(axis.text.x=element_blank(), axis.ticks.x=element_blank())
# Density plot
ggplot(training, aes(x = TEAM_BASERUN_SB)) + 
  geom_histogram(aes(y=..density..), colour="black", fill="white",bins=50) +
  geom_density(alpha=.2, fill="#FF6666") + ylab("") + xlab("Density Plot with Mean") +
  geom_vline(aes(xintercept=mean(TEAM_BASERUN_SB, na.rm=TRUE)), color="red", linetype="dashed", size=1)
# Scatterplot
ggplot(data=training, aes(x=TEAM_BASERUN_SB, y=TARGET_WINS)) + 
  geom_point() + geom_smooth(method = "loess") +
  xlab("Scatterplot with Best Fit Line")

```

**Data Overview:** The range and distribution are reasonable. The only issue are 133 records with missing or zero value. These values can be imputed in order to use these records in model building.


#### TEAM_BASERUN_CS: 

This variable represents Number of team runners caught stealing

```{r echo=FALSE, warning=FALSE, message=FALSE, fig.width=10, fig.height=6}
sumtable[sumtable[,1]=="TEAM_BASERUN_CS",2:8]
# Boxplot
ggplot(training, aes(x = 1, y = TEAM_BASERUN_CS)) + 
  stat_boxplot(geom ='errorbar') + geom_boxplot() + 
  xlab("Boxplot") + ylab("") + theme(axis.text.x=element_blank(), axis.ticks.x=element_blank())
# Density plot
ggplot(training, aes(x = TEAM_BASERUN_CS)) + 
  geom_histogram(aes(y=..density..), colour="black", fill="white",bins=50) +
  geom_density(alpha=.2, fill="#FF6666") + ylab("") + xlab("Density Plot with Mean") +
  geom_vline(aes(xintercept=mean(TEAM_BASERUN_CS, na.rm=TRUE)), color="red", linetype="dashed", size=1)
# Scatterplot
ggplot(data=training, aes(x=TEAM_BASERUN_CS, y=TARGET_WINS)) + 
  geom_point() + geom_smooth(method = "loess") +
  xlab("Scatterplot with Best Fit Line")

```

**Data Overview:** The range and distribution are reasonable; however, there is significant number of missing values - 773, including one zero value. This represents a third of the entire data set. It may be possible to impute this value, but it may be necessary to leave this variable out of model building.



#### TEAM_FIELDING_E: 

This variable represents  Number of team fielding errors

```{r echo=FALSE, warning=FALSE, message=FALSE, fig.width=10, fig.height=6}
sumtable[sumtable[,1]=="TEAM_FIELDING_E",2:8]
# Boxplot
ggplot(training, aes(x = 1, y = TEAM_FIELDING_E)) + 
  stat_boxplot(geom ='errorbar') + geom_boxplot() + 
  xlab("Boxplot") + ylab("") + theme(axis.text.x=element_blank(), axis.ticks.x=element_blank())
# Density plot
ggplot(training, aes(x = TEAM_FIELDING_E)) + 
  geom_histogram(aes(y=..density..), colour="black", fill="white",bins=50) +
  geom_density(alpha=.2, fill="#FF6666") + ylab("") + xlab("Density Plot with Mean") +
  geom_vline(aes(xintercept=mean(TEAM_FIELDING_E, na.rm=TRUE)), color="red", linetype="dashed", size=1)
# Scatterplot
ggplot(data=training, aes(x=TEAM_FIELDING_E, y=TARGET_WINS)) + 
  geom_point() + geom_smooth(method = "loess") +
  xlab("Scatterplot with Best Fit Line")

```

**Data Overview:** There are no missing values. Distribution has a very long right tail. Values in the 1,000 and above range are highly suspect. One of the highest historical number of errors is 867 errors by Washington in 1886 for 122 games. That is equal about 1,151 errors for 162 game season. There are multiple values above that number. This may unfavorably influence a model.


#### TEAM_FIELDING_DP: 

This variable represents Number of team fielding double plays

```{r echo=FALSE, warning=FALSE, message=FALSE, fig.width=10, fig.height=6}
sumtable[sumtable[,1]=="TEAM_FIELDING_DP",2:8]
# Boxplot
ggplot(training, aes(x = 1, y = TEAM_FIELDING_DP)) + 
  stat_boxplot(geom ='errorbar') + geom_boxplot() + 
  xlab("Boxplot") + ylab("") + theme(axis.text.x=element_blank(), axis.ticks.x=element_blank())
# Density plot
ggplot(training, aes(x = TEAM_FIELDING_DP)) + 
  geom_histogram(aes(y=..density..), colour="black", fill="white",bins=50) +
  geom_density(alpha=.2, fill="#FF6666") + ylab("") + xlab("Density Plot with Mean") +
  geom_vline(aes(xintercept=mean(TEAM_FIELDING_DP, na.rm=TRUE)), color="red", linetype="dashed", size=1)
# Scatterplot
ggplot(data=training, aes(x=TEAM_FIELDING_DP, y=TARGET_WINS)) + 
  geom_point() + geom_smooth(method = "loess") +
  xlab("Scatterplot with Best Fit Line")

```

**Data Overview:** The range and distribution are reasonable. Similar to a few other variables there is a medium number off missing values - 286 records. This value can be imputed.


#### TEAM_PITCHING_BB: 

This variable represents Number of walks given up by pitchers

```{r echo=FALSE, warning=FALSE, message=FALSE, fig.width=10, fig.height=6}
sumtable[sumtable[,1]=="TEAM_PITCHING_BB",2:8]
# Boxplot
ggplot(training, aes(x = 1, y = TEAM_PITCHING_BB)) + 
  stat_boxplot(geom ='errorbar') + geom_boxplot() + 
  xlab("Boxplot") + ylab("") + theme(axis.text.x=element_blank(), axis.ticks.x=element_blank())
# Density plot
ggplot(training, aes(x = TEAM_PITCHING_BB)) + 
  geom_histogram(aes(y=..density..), colour="black", fill="white",bins=50) +
  geom_density(alpha=.2, fill="#FF6666") + ylab("") + xlab("Density Plot with Mean") +
  geom_vline(aes(xintercept=mean(TEAM_PITCHING_BB, na.rm=TRUE)), color="red", linetype="dashed", size=1)
# Scatterplot
ggplot(data=training, aes(x=TEAM_PITCHING_BB, y=TARGET_WINS)) + 
  geom_point() + geom_smooth(method = "loess") +
  xlab("Scatterplot with Best Fit Line")

```

**Data Overview:**  There are no missing values with the exception of record 1347 which will be deleted from model building. There are some unrealistic outliers.


#### TEAM_PITCHING_H: 

This variable represents Number of base hits given up by pitchers

```{r echo=FALSE, warning=FALSE, message=FALSE, fig.width=10, fig.height=6}
sumtable[sumtable[,1]=="TEAM_PITCHING_H",2:8]
# Boxplot
ggplot(training, aes(x = 1, y = TEAM_PITCHING_H)) + 
  stat_boxplot(geom ='errorbar') + geom_boxplot() + 
  xlab("Boxplot") + ylab("") + theme(axis.text.x=element_blank(), axis.ticks.x=element_blank())
# Density plot
ggplot(training, aes(x = TEAM_PITCHING_H)) + 
  geom_histogram(aes(y=..density..), colour="black", fill="white",bins=50) +
  geom_density(alpha=.2, fill="#FF6666") + ylab("") + xlab("Density Plot with Mean") +
  geom_vline(aes(xintercept=mean(TEAM_PITCHING_H, na.rm=TRUE)), color="red", linetype="dashed", size=1)
# Scatterplot
ggplot(data=training, aes(x=TEAM_PITCHING_H, y=TARGET_WINS)) + 
  geom_point() + geom_smooth(method = "loess") +
  xlab("Scatterplot with Best Fit Line")

```

**Data Overview:** Similar to `TEAM_PITCHING_BB` above, there are no missing value, but there issues with outliers. Based on visualizations, this variable will be capped at 13,000 and any value over this will be set to this cap.

#### TEAM_PITCHING_SO:

This variable represents Number of strikeouts by pitchers

```{r echo=FALSE, warning=FALSE, message=FALSE, fig.width=10, fig.height=6}
sumtable[sumtable[,1]=="TEAM_PITCHING_SO",2:8]
# Boxplot
ggplot(training, aes(x = 1, y = TEAM_PITCHING_SO)) + 
  stat_boxplot(geom ='errorbar') + geom_boxplot() + 
  xlab("Boxplot") + ylab("") + theme(axis.text.x=element_blank(), axis.ticks.x=element_blank())
# Density plot
ggplot(training, aes(x = TEAM_PITCHING_SO)) + 
  geom_histogram(aes(y=..density..), colour="black", fill="white",bins=50) +
  geom_density(alpha=.2, fill="#FF6666") + ylab("") + xlab("Density Plot with Mean") +
  geom_vline(aes(xintercept=mean(TEAM_PITCHING_SO, na.rm=TRUE)), color="red", linetype="dashed", size=1)
# Scatterplot
ggplot(data=training, aes(x=TEAM_PITCHING_SO, y=TARGET_WINS)) + 
  geom_point() + geom_smooth(method = "loess") +
  xlab("Scatterplot with Best Fit Line")

```

**Data Overview:** This variable has 122 missing or zero values. They can be imputed as needed. There is also an outlier issue as graph shows.

#### TARGET_WINS: 

This variable represents Number of wins **(Outcome)**

```{r echo=FALSE, warning=FALSE, message=FALSE, fig.width=10, fig.height=6}
sumtable[sumtable[,1]=="TARGET_WINS",2:8]
# Boxplot
ggplot(training, aes(x = 1, y = TARGET_WINS)) + 
  stat_boxplot(geom ='errorbar') + geom_boxplot() + 
  xlab("Boxplot") + ylab("") + theme(axis.text.x=element_blank(), axis.ticks.x=element_blank())
# Density plot
ggplot(training, aes(x = TARGET_WINS)) + 
  geom_histogram(aes(y=..density..), colour="black", fill="white",bins=50) +
  geom_density(alpha=.2, fill="#FF6666") + ylab("") + xlab("Density Plot with Mean") +
  geom_vline(aes(xintercept=mean(TARGET_WINS, na.rm=TRUE)), color="red", linetype="dashed", size=1)

```

**Data Overview:** The range and distribution are reasonable. There are no missing values with the exception of record 1347.

## DATA PREPARATION

### Fixing Missing/Zero Values- TRAINING DATA

First I will remove the invalid data and prep it for imputation.  I will drop the hit by pitcher variable from the dataset.

```{r echo=FALSE, warning=FALSE, message=FALSE}
clean_data <- function(df){
  # Change 0's to NA so they too can be imputed
  df <- df %>%
  mutate(TEAM_BATTING_SO = ifelse(TEAM_BATTING_SO == 0, NA, TEAM_BATTING_SO))
  # Remove the high pitching strikeout values
  df[which(df$TEAM_PITCHING_SO > 5346),"TEAM_PITCHING_SO"] <- NA
  # Drop the hit by pitcher variable
  df %>%
    select(-TEAM_BATTING_HBP)
}
training <- clean_data(training)
```

### KNN imputation - TRAINING DATA

```{r echo=FALSE, warning=FALSE, message=FALSE}
set.seed(42)
knn <- training %>% knnImputation()
apply_func <- function(df, knn){
  impute_me <- is.na(df$TEAM_BATTING_SO)
  df[impute_me,"TEAM_BATTING_SO"] <- knn[impute_me,"TEAM_BATTING_SO"] 
  impute_me <- is.na(df$TEAM_BASERUN_SB)
  df[impute_me,"TEAM_BASERUN_SB"] <- knn[impute_me,"TEAM_BASERUN_SB"] 
  impute_me <- is.na(df$TEAM_BASERUN_CS)
  df[impute_me,"TEAM_BASERUN_CS"] <- knn[impute_me,"TEAM_BASERUN_CS"] 
  impute_me <- is.na(df$TEAM_PITCHING_SO)
  df[impute_me,"TEAM_PITCHING_SO"] <- knn[impute_me,"TEAM_PITCHING_SO"]
  impute_me <- is.na(df$TEAM_FIELDING_DP)
  df[impute_me,"TEAM_FIELDING_DP"] <- knn[impute_me,"TEAM_FIELDING_DP"]
  return(df)
}
training <- apply_func(training, knn)
```
### Feature Engineering - TRAINING DATA

The batting singles is not included but I can back it out of the hits. 

```{r echo=FALSE, warning=FALSE, message=FALSE}
add_features <- function(df){
  df %>%
    mutate(TEAM_BATTING_1B = TEAM_BATTING_H - TEAM_BATTING_2B - TEAM_BATTING_3B - TEAM_BATTING_HR)
}
training <- add_features(training)

```
### Model Data Look

Here's what the data look like after imputation and correction:

```{r  echo=FALSE, warning=FALSE, message=FALSE}
training %>%
  gather(variable, value) %>%
  ggplot(., aes(value)) + 
  geom_density(fill = "indianred4", color="indianred4") + 
  facet_wrap(~variable, scales ="free", ncol = 4) +
  labs(x = element_blank(), y = element_blank())
```

```{r  echo=FALSE, warning=FALSE, message=FALSE}
quick_summary <- function(df){
  df %>%
    summary() 
}
quick_summary(training)
```

## BUILD MODELS

I split training dat 70 %training purposes and 30 % for testing purposes.

```{r echo=FALSE, warning=FALSE, message=FALSE}
set.seed(42)
train_index <- createDataPartition(training$TARGET_WINS, p = .7, list = FALSE, times = 1)
train <- training[train_index,]
test <- training[-train_index,]
```

### Model 1

The first model includes several variables, selected manually, that have higher than average correlation to the target variable. They cover hitting, walking and fielding errors.

```{r echo=FALSE, warning=FALSE, message=FALSE}
m1 <- lm(TARGET_WINS ~ TEAM_BATTING_H + TEAM_BATTING_BB + TEAM_FIELDING_E, data=train)
summary(m1)
```

All variables are significant, but the $R^2$ value is relatively small at 0.2427.

### Model 2

The second model expand the base hit variable, `TEAM_BATTING_H`, into its components - singles, doubles, triples and home runs. 

```{r echo=FALSE, warning=FALSE, message=FALSE}
m2 <- lm(TARGET_WINS ~ TEAM_BATTING_1B + TEAM_BATTING_2B + TEAM_BATTING_3B + TEAM_BATTING_HR + 
           TEAM_BATTING_BB + TEAM_FIELDING_E, data=train)
summary(m2)
```

All variables are still significant and $R^2$ is slightly improved at 0.2628.


### Model 3 :Higher Order Stepwise Regression

For the third model I will use a stepwise regression method using a backwards elimination process.  I also introduce some higher order polynomial variables.

```{r  echo=FALSE, warning=FALSE, message=FALSE}
full_formula <- "TARGET_WINS ~ TEAM_BATTING_2B + TEAM_BATTING_3B + TEAM_BATTING_HR + TEAM_BATTING_BB + TEAM_BATTING_SO + TEAM_BASERUN_SB + TEAM_BASERUN_CS + TEAM_PITCHING_H + TEAM_PITCHING_HR + TEAM_PITCHING_BB + TEAM_PITCHING_SO + TEAM_FIELDING_E + TEAM_FIELDING_DP + TEAM_BATTING_1B + I(TEAM_BATTING_2B^2) + I(TEAM_BATTING_3B^2) + I(TEAM_BATTING_HR^2) + I(TEAM_BATTING_BB^2) + I(TEAM_BATTING_SO^2) + I(TEAM_BASERUN_SB^2) + I(TEAM_BASERUN_CS^2) + I(TEAM_PITCHING_H^2) + I(TEAM_PITCHING_HR^2) + I(TEAM_PITCHING_BB^2) + I(TEAM_PITCHING_SO^2) + I(TEAM_FIELDING_E^2) + I(TEAM_FIELDING_DP^2) + I(TEAM_BATTING_1B^2) "
full_model <- lm(full_formula, train)
step_back <- MASS::stepAIC(full_model, direction="backward", trace = F)
poly_call <- summary(step_back)$call
m3 <- lm(poly_call[2], train)
summary(m3)
```
This model has the highest adjusted R-squared value at 0.3944  .Some variables p-values are not in 95 % siginificant level but they are in 90 % significant level which is acceptable.

### Model 4

For the fourth model,  Variables were selected either based on  correlation information from the first section.
The following model has $R^2$ values of 0.2606, which is relatively close to the fourth model; however, this model has fewer variables and may be preferential because of its simplicity. 

```{r echo=FALSE, warning=FALSE, message=FALSE}
# Create log fielding error
train$TEAM_FIELDING_E_LOG <- log(train$TEAM_FIELDING_E)
m4 <- lm(TARGET_WINS ~ TEAM_BATTING_SO  + TEAM_BATTING_3B + 
           TEAM_BATTING_HR + TEAM_BASERUN_SB  + 
           TEAM_FIELDING_E_LOG*TEAM_PITCHING_H, data=train)
summary(m4)
```

## SELECT MODELS

In order to select which model is the "best" I will test it against a validation (test) set.  I will examine the difference between the predicted and actual values.

### Model 1  Results

```{r  echo=FALSE, warning=FALSE, message=FALSE}
test$m1 <- predict(m1, test)
test <- test %>%
  mutate(m1_error = TARGET_WINS - m1)
ggplot(test, aes(m1_error)) +
  geom_histogram(bins = 50) +
  annotate("text",x=0,y=10,
           label = paste("RMSE = ",
                         round(sqrt(mean(test$m1_error^2)),2)
                         ),
           color="white"
           )
summary(test$m1_error)
```


### Model 2  Results

```{r  echo=FALSE, warning=FALSE, message=FALSE}
test$m2 <- predict(m2, test)
test <- test %>%
  mutate(m2_error = TARGET_WINS - m2)
ggplot(test, aes(m2_error)) +
  geom_histogram(bins = 50) +
  annotate("text",x=0,y=10,
           label = paste("RMSE = ",
                         round(sqrt(mean(test$m2_error^2)),2)
                         ),
           color="white"
           )
summary(test$m2_error)
```

### Model 3  Results

```{r echo=FALSE, warning=FALSE, message=FALSE}
test$m3 <- predict(m3, test)
test <- test %>%
  mutate(m3_error = TARGET_WINS - m3)
ggplot(test, aes(m3_error)) +
  geom_histogram(bins = 50) +
  annotate("text",x=0,y=10,
           label = paste("RMSE = ",
                         round(sqrt(mean(test$m3_error^2)),2)
                         ),
           color="white"
           )
summary(test$m3_error)
```

### Model 4  Results

```{r  echo=FALSE, warning=FALSE, message=FALSE}
# Create log fielding error
test$TEAM_FIELDING_E_LOG <- log(test$TEAM_FIELDING_E)
test$m4 <- predict(m4, test)
test <- test %>%
  mutate(m4_error = TARGET_WINS - m4)
ggplot(test, aes(m4_error)) +
  geom_histogram(bins = 50) +
  annotate("text",x=0,y=10,
           label = paste("RMSE = ",
                         round(sqrt(mean(test$m4_error^2)),2)
                         ),
           color="white"
           )
summary(test$m4_error)
```


**Based on $R^2$ value and RMSE results, the third model (M3) was selected for further analysis. This model also has the lowest AIC score.** 

```{r echo=FALSE, warning=FALSE, message=FALSE}
AIC(m1, m2, m3, m4)
summary(m3)
```


```{r echo=FALSE, warning=FALSE, message=FALSE}
plot(m3$residuals, ylab="Residuals")
abline(h=0)
```

## Prediction

In order to make prediction, I need to impute missing values on "evalution" dataset by using the same as training imputation.

### Fixing Missing/Zero Values

First I will remove the invalid data and prep it for imputation.  I will drop the hit by pitcher variable from the dataset.

```{r  echo=FALSE, warning=FALSE, message=FALSE}
clean_data <- function(df){
  # Change 0's to NA so they too can be imputed
  df <- df %>%
  mutate(TEAM_BATTING_SO = ifelse(TEAM_BATTING_SO == 0, NA, TEAM_BATTING_SO))
  # Remove the high pitching strikeout values
  df[which(df$TEAM_PITCHING_SO > 5346),"TEAM_PITCHING_SO"] <- NA
  # Drop the hit by pitcher variable
  df %>%
    select(-TEAM_BATTING_HBP)
}
evaluation <- clean_data(evaluation)
```

### KNN imputation 

```{r  echo=FALSE, warning=FALSE, message=FALSE}
set.seed(42)
knn <- evaluation %>% knnImputation()
apply_func <- function(df, knn){
  impute_me <- is.na(df$TEAM_BATTING_SO)
  df[impute_me,"TEAM_BATTING_SO"] <- knn[impute_me,"TEAM_BATTING_SO"] 
  impute_me <- is.na(df$TEAM_BASERUN_SB)
  df[impute_me,"TEAM_BASERUN_SB"] <- knn[impute_me,"TEAM_BASERUN_SB"] 
  impute_me <- is.na(df$TEAM_BASERUN_CS)
  df[impute_me,"TEAM_BASERUN_CS"] <- knn[impute_me,"TEAM_BASERUN_CS"] 
  impute_me <- is.na(df$TEAM_PITCHING_SO)
  df[impute_me,"TEAM_PITCHING_SO"] <- knn[impute_me,"TEAM_PITCHING_SO"]
  impute_me <- is.na(df$TEAM_FIELDING_DP)
  df[impute_me,"TEAM_FIELDING_DP"] <- knn[impute_me,"TEAM_FIELDING_DP"]
  return(df)
}
evaluation <- apply_func(evaluation, knn)
```
### Feature Engineering 

The batting singles is not included but I can back it out of the hits.

```{r  echo=FALSE, warning=FALSE, message=FALSE}
add_features <- function(df){
  df %>%
    mutate(TEAM_BATTING_1B = TEAM_BATTING_H - TEAM_BATTING_2B - TEAM_BATTING_3B - TEAM_BATTING_HR)
}
evaluation <- add_features(evaluation)

```

### Prediction 

```{r echo=FALSE, warning=FALSE, message=FALSE}
evaluation$PREDICT_WIN <- predict(m3, newdata=evaluation, interval="confidence")
Forecast <- cbind(evaluation$INDEX, evaluation$PREDICT_WIN[, 1], evaluation$PREDICT_WIN[, 2], evaluation$PREDICT_WIN[, 3])
colnames(Forecast) <- c("Index", "Predicted Wins", "CI Lower", "CI Upper")
round(Forecast,0)
```


## APPENDIX: R Script


# Read in the training data
training <- read.csv("https://raw.githubusercontent.com/omerozeren/DATA621/master/moneyball-training-data.csv") 
# Read in the evaluation data
evaluation <- read.csv("https://raw.githubusercontent.com/omerozeren/DATA621/master/moneyball-evaluation-data.csv")




sumtable = data.frame(Variable = character(),
                   Min = integer(),
                   Median = integer(),
                   Mean = double(),
                   SD = double(),
                   Max = integer(),
                   Num_Zeros = integer(),
                   Num_NaN = integer())
for (i in 2:17) {
  sumtable <- rbind(sumtable, data.frame(Variable = colnames(training)[i],
                                   Min = min(training[,i], na.rm=TRUE),
                                   Median = median(training[,i], na.rm=TRUE),
                                   Mean = round(mean(training[,i], na.rm=TRUE)),
                                   SD = round(sd(training[,i], na.rm=TRUE)),
                                   Max = max(training[,i], na.rm=TRUE),
                                   Num_Zeros = length(which(training[,i]==0)),
                                   Num_NaN = sum(is.na(training[,i])))
                 )
}
colnames(sumtable) <- c("", "Min", "Median", "Mean", "SD", "Max","Num_Zeros", "Num_NaN")
sumtable




cm <- cor(training, use="pairwise.complete.obs")
cm <- cm[2:17,2:17]
names <- c("Wins", "H", "2B", "3B", "HR", "BB", "SO", "SB", "CS", "HBP", "P-H", "P-HR", "P-BB", "P-SO", "E", "DP")
colnames(cm) <- names; rownames(cm) <- names
round(cm,2)


cm <- cor(training, use="pairwise.complete.obs")
cm <- cm[2:17,2:17]
names <- c("Wins", "H", "2B", "3B", "HR", "BB", "SO", "SB", "CS", "HBP", "P-H", "P-HR", "P-BB", "P-SO", "E", "DP")
colnames(cm) <- names; rownames(cm) <- names
corrplot(cm, method = "color", type = "upper", tl.col = "black", diag = FALSE)

training %>%
  gather(variable, value, -TARGET_WINS) %>%
  ggplot(., aes(value, TARGET_WINS)) + 
  geom_point(fill = "indianred4", color="indianred4") + 
  geom_smooth(method = "lm", se = FALSE, color = "black") + 
  facet_wrap(~variable, scales ="free", ncol = 4) +
  labs(x = element_blank(), y = "Wins")



sumtable[sumtable[,1]=="TEAM_BATTING_H",2:8]

# Boxplot
ggplot(training, aes(x = 1, y = TEAM_BATTING_H)) + 
  stat_boxplot(geom ='errorbar') + geom_boxplot() + 
  xlab("Boxplot") + ylab("") + theme(axis.text.x=element_blank(), axis.ticks.x=element_blank())

# Density plot
ggplot(training, aes(x = TEAM_BATTING_H)) + 
  geom_histogram(aes(y=..density..), colour="black", fill="white",bins=50) +
  geom_density(alpha=.2, fill="#FF6666") + ylab("") + xlab("Density Plot with Mean") +
  geom_vline(aes(xintercept=mean(TEAM_BATTING_H, na.rm=TRUE)), color="red", linetype="dashed", size=1)

# Scatterplot
ggplot(data=training, aes(x=TEAM_BATTING_H, y=TARGET_WINS)) + 
  geom_point() + geom_smooth(method = "loess") +
  xlab("Scatterplot with Best Fit Line")



#### TEAM_BATTING_2B:


sumtable[sumtable[,1]=="TEAM_BATTING_2B",2:8]
# Boxplot
ggplot(training, aes(x = 1, y = TEAM_BATTING_2B)) + 
  stat_boxplot(geom ='errorbar') + geom_boxplot() + 
  xlab("Boxplot") + ylab("") + theme(axis.text.x=element_blank(), axis.ticks.x=element_blank())
# Density plot
ggplot(training, aes(x = TEAM_BATTING_2B)) + 
  geom_histogram(aes(y=..density..), colour="black", fill="white",bins=50) +
  geom_density(alpha=.2, fill="#FF6666") + ylab("") + xlab("Density Plot with Mean") +
  geom_vline(aes(xintercept=mean(TEAM_BATTING_2B, na.rm=TRUE)), color="red", linetype="dashed", size=1)
# Scatterplot
ggplot(data=training, aes(x=TEAM_BATTING_2B, y=TARGET_WINS)) + 
  geom_point() + geom_smooth(method = "loess") +
  xlab("Scatterplot with Best Fit Line")



#### TEAM_BATTING_3B: 


sumtable[sumtable[,1]=="TEAM_BATTING_3B",2:8]
# Boxplot
ggplot(training, aes(x = 1, y = TEAM_BATTING_3B)) + 
  stat_boxplot(geom ='errorbar') + geom_boxplot() + 
  xlab("Boxplot") + ylab("") + theme(axis.text.x=element_blank(), axis.ticks.x=element_blank())
# Density plot
ggplot(training, aes(x = TEAM_BATTING_3B)) + 
  geom_histogram(aes(y=..density..), colour="black", fill="white",bins=50) +
  geom_density(alpha=.2, fill="#FF6666") + ylab("") + xlab("Density Plot with Mean") +
  geom_vline(aes(xintercept=mean(TEAM_BATTING_3B, na.rm=TRUE)), color="red", linetype="dashed", size=1)
# Scatterplot
ggplot(data=training, aes(x=TEAM_BATTING_3B, y=TARGET_WINS)) + 
  geom_point() + geom_smooth(method = "loess") +
  xlab("Scatterplot with Best Fit Line")


#### TEAM_BATTING_HR: 


sumtable[sumtable[,1]=="TEAM_BATTING_HR",2:8]
# Boxplot
ggplot(training, aes(x = 1, y = TEAM_BATTING_HR)) + 
  stat_boxplot(geom ='errorbar') + geom_boxplot() + 
  xlab("Boxplot") + ylab("") + theme(axis.text.x=element_blank(), axis.ticks.x=element_blank())
# Density plot
ggplot(training, aes(x = TEAM_BATTING_HR)) + 
  geom_histogram(aes(y=..density..), colour="black", fill="white",bins=50) +
  geom_density(alpha=.2, fill="#FF6666") + ylab("") + xlab("Density Plot with Mean") +
  geom_vline(aes(xintercept=mean(TEAM_BATTING_HR, na.rm=TRUE)), color="red", linetype="dashed", size=1)
# Scatterplot
ggplot(data=training, aes(x=TEAM_BATTING_HR, y=TARGET_WINS)) + 
  geom_point() + geom_smooth(method = "loess") +
  xlab("Scatterplot with Best Fit Line")


#### TEAM_BATTING_BB: 

This variable represents Number of team walks


sumtable[sumtable[,1]=="TEAM_BATTING_BB",2:8]
# Boxplot
ggplot(training, aes(x = 1, y = TEAM_BATTING_BB)) + 
  stat_boxplot(geom ='errorbar') + geom_boxplot() + 
  xlab("Boxplot") + ylab("") + theme(axis.text.x=element_blank(), axis.ticks.x=element_blank())
# Density plot
ggplot(training, aes(x = TEAM_BATTING_BB)) + 
  geom_histogram(aes(y=..density..), colour="black", fill="white",bins=50) +
  geom_density(alpha=.2, fill="#FF6666") + ylab("") + xlab("Density Plot with Mean") +
  geom_vline(aes(xintercept=mean(TEAM_BATTING_BB, na.rm=TRUE)), color="red", linetype="dashed", size=1)
# Scatterplot
ggplot(data=training, aes(x=TEAM_BATTING_BB, y=TARGET_WINS)) + 
  geom_point() + geom_smooth(method = "loess") +
  xlab("Scatterplot with Best Fit Line")


#### TEAM_BATTING_HBP: 

sumtable[sumtable[,1]=="TEAM_BATTING_HBP",2:8]
# Boxplot
ggplot(training, aes(x = 1, y = TEAM_BATTING_HBP)) + 
  stat_boxplot(geom ='errorbar') + geom_boxplot() + 
  xlab("Boxplot") + ylab("") + theme(axis.text.x=element_blank(), axis.ticks.x=element_blank())
# Density plot
ggplot(training, aes(x = TEAM_BATTING_HBP)) + 
  geom_histogram(aes(y=..density..), colour="black", fill="white",bins=50) +
  geom_density(alpha=.2, fill="#FF6666") + ylab("") + xlab("Density Plot with Mean") +
  geom_vline(aes(xintercept=mean(TEAM_BATTING_HBP, na.rm=TRUE)), color="red", linetype="dashed", size=1)
# Scatterplot
ggplot(data=training, aes(x=TEAM_BATTING_HBP, y=TARGET_WINS)) + 
  geom_point() + geom_smooth(method = "loess") +
  xlab("Scatterplot with Best Fit Line")

#### TEAM_BATTING_SO: 


sumtable[sumtable[,1]=="TEAM_BATTING_SO",2:8]
# Boxplot
ggplot(training, aes(x = 1, y = TEAM_BATTING_SO)) + 
  stat_boxplot(geom ='errorbar') + geom_boxplot() + 
  xlab("Boxplot") + ylab("") + theme(axis.text.x=element_blank(), axis.ticks.x=element_blank())
# Density plot
ggplot(training, aes(x = TEAM_BATTING_SO)) + 
  geom_histogram(aes(y=..density..), colour="black", fill="white",bins=50) +
  geom_density(alpha=.2, fill="#FF6666") + ylab("") + xlab("Density Plot with Mean") +
  geom_vline(aes(xintercept=mean(TEAM_BATTING_SO, na.rm=TRUE)), color="red", linetype="dashed", size=1)
# Scatterplot
ggplot(data=training, aes(x=TEAM_BATTING_SO, y=TARGET_WINS)) + 
  geom_point() + geom_smooth(method = "loess") +
  xlab("Scatterplot with Best Fit Line")

#### TEAM_BASERUN_SB: 


sumtable[sumtable[,1]=="TEAM_BASERUN_SB",2:8]
# Boxplot
ggplot(training, aes(x = 1, y = TEAM_BASERUN_SB)) + 
  stat_boxplot(geom ='errorbar') + geom_boxplot() + 
  xlab("Boxplot") + ylab("") + theme(axis.text.x=element_blank(), axis.ticks.x=element_blank())
# Density plot
ggplot(training, aes(x = TEAM_BASERUN_SB)) + 
  geom_histogram(aes(y=..density..), colour="black", fill="white",bins=50) +
  geom_density(alpha=.2, fill="#FF6666") + ylab("") + xlab("Density Plot with Mean") +
  geom_vline(aes(xintercept=mean(TEAM_BASERUN_SB, na.rm=TRUE)), color="red", linetype="dashed", size=1)
# Scatterplot
ggplot(data=training, aes(x=TEAM_BASERUN_SB, y=TARGET_WINS)) + 
  geom_point() + geom_smooth(method = "loess") +
  xlab("Scatterplot with Best Fit Line")


#### TEAM_BASERUN_CS: 

sumtable[sumtable[,1]=="TEAM_BASERUN_CS",2:8]
# Boxplot
ggplot(training, aes(x = 1, y = TEAM_BASERUN_CS)) + 
  stat_boxplot(geom ='errorbar') + geom_boxplot() + 
  xlab("Boxplot") + ylab("") + theme(axis.text.x=element_blank(), axis.ticks.x=element_blank())
# Density plot
ggplot(training, aes(x = TEAM_BASERUN_CS)) + 
  geom_histogram(aes(y=..density..), colour="black", fill="white",bins=50) +
  geom_density(alpha=.2, fill="#FF6666") + ylab("") + xlab("Density Plot with Mean") +
  geom_vline(aes(xintercept=mean(TEAM_BASERUN_CS, na.rm=TRUE)), color="red", linetype="dashed", size=1)
# Scatterplot
ggplot(data=training, aes(x=TEAM_BASERUN_CS, y=TARGET_WINS)) + 
  geom_point() + geom_smooth(method = "loess") +
  xlab("Scatterplot with Best Fit Line")



#### TEAM_FIELDING_E: 


sumtable[sumtable[,1]=="TEAM_FIELDING_E",2:8]
# Boxplot
ggplot(training, aes(x = 1, y = TEAM_FIELDING_E)) + 
  stat_boxplot(geom ='errorbar') + geom_boxplot() + 
  xlab("Boxplot") + ylab("") + theme(axis.text.x=element_blank(), axis.ticks.x=element_blank())
# Density plot
ggplot(training, aes(x = TEAM_FIELDING_E)) + 
  geom_histogram(aes(y=..density..), colour="black", fill="white",bins=50) +
  geom_density(alpha=.2, fill="#FF6666") + ylab("") + xlab("Density Plot with Mean") +
  geom_vline(aes(xintercept=mean(TEAM_FIELDING_E, na.rm=TRUE)), color="red", linetype="dashed", size=1)
# Scatterplot
ggplot(data=training, aes(x=TEAM_FIELDING_E, y=TARGET_WINS)) + 
  geom_point() + geom_smooth(method = "loess") +
  xlab("Scatterplot with Best Fit Line")


#### TEAM_FIELDING_DP: 

sumtable[sumtable[,1]=="TEAM_FIELDING_DP",2:8]
# Boxplot
ggplot(training, aes(x = 1, y = TEAM_FIELDING_DP)) + 
  stat_boxplot(geom ='errorbar') + geom_boxplot() + 
  xlab("Boxplot") + ylab("") + theme(axis.text.x=element_blank(), axis.ticks.x=element_blank())
# Density plot
ggplot(training, aes(x = TEAM_FIELDING_DP)) + 
  geom_histogram(aes(y=..density..), colour="black", fill="white",bins=50) +
  geom_density(alpha=.2, fill="#FF6666") + ylab("") + xlab("Density Plot with Mean") +
  geom_vline(aes(xintercept=mean(TEAM_FIELDING_DP, na.rm=TRUE)), color="red", linetype="dashed", size=1)
# Scatterplot
ggplot(data=training, aes(x=TEAM_FIELDING_DP, y=TARGET_WINS)) + 
  geom_point() + geom_smooth(method = "loess") +
  xlab("Scatterplot with Best Fit Line")


#### TEAM_PITCHING_BB: 


sumtable[sumtable[,1]=="TEAM_PITCHING_BB",2:8]
# Boxplot
ggplot(training, aes(x = 1, y = TEAM_PITCHING_BB)) + 
  stat_boxplot(geom ='errorbar') + geom_boxplot() + 
  xlab("Boxplot") + ylab("") + theme(axis.text.x=element_blank(), axis.ticks.x=element_blank())
# Density plot
ggplot(training, aes(x = TEAM_PITCHING_BB)) + 
  geom_histogram(aes(y=..density..), colour="black", fill="white",bins=50) +
  geom_density(alpha=.2, fill="#FF6666") + ylab("") + xlab("Density Plot with Mean") +
  geom_vline(aes(xintercept=mean(TEAM_PITCHING_BB, na.rm=TRUE)), color="red", linetype="dashed", size=1)
# Scatterplot
ggplot(data=training, aes(x=TEAM_PITCHING_BB, y=TARGET_WINS)) + 
  geom_point() + geom_smooth(method = "loess") +
  xlab("Scatterplot with Best Fit Line")


#### TEAM_PITCHING_H: 

sumtable[sumtable[,1]=="TEAM_PITCHING_H",2:8]
# Boxplot
ggplot(training, aes(x = 1, y = TEAM_PITCHING_H)) + 
  stat_boxplot(geom ='errorbar') + geom_boxplot() + 
  xlab("Boxplot") + ylab("") + theme(axis.text.x=element_blank(), axis.ticks.x=element_blank())
# Density plot
ggplot(training, aes(x = TEAM_PITCHING_H)) + 
  geom_histogram(aes(y=..density..), colour="black", fill="white",bins=50) +
  geom_density(alpha=.2, fill="#FF6666") + ylab("") + xlab("Density Plot with Mean") +
  geom_vline(aes(xintercept=mean(TEAM_PITCHING_H, na.rm=TRUE)), color="red", linetype="dashed", size=1)
# Scatterplot
ggplot(data=training, aes(x=TEAM_PITCHING_H, y=TARGET_WINS)) + 
  geom_point() + geom_smooth(method = "loess") +
  xlab("Scatterplot with Best Fit Line")

#### TEAM_PITCHING_SO:

sumtable[sumtable[,1]=="TEAM_PITCHING_SO",2:8]
# Boxplot
ggplot(training, aes(x = 1, y = TEAM_PITCHING_SO)) + 
  stat_boxplot(geom ='errorbar') + geom_boxplot() + 
  xlab("Boxplot") + ylab("") + theme(axis.text.x=element_blank(), axis.ticks.x=element_blank())
# Density plot
ggplot(training, aes(x = TEAM_PITCHING_SO)) + 
  geom_histogram(aes(y=..density..), colour="black", fill="white",bins=50) +
  geom_density(alpha=.2, fill="#FF6666") + ylab("") + xlab("Density Plot with Mean") +
  geom_vline(aes(xintercept=mean(TEAM_PITCHING_SO, na.rm=TRUE)), color="red", linetype="dashed", size=1)
# Scatterplot
ggplot(data=training, aes(x=TEAM_PITCHING_SO, y=TARGET_WINS)) + 
  geom_point() + geom_smooth(method = "loess") +
  xlab("Scatterplot with Best Fit Line")

#### TARGET_WINS: 


sumtable[sumtable[,1]=="TARGET_WINS",2:8]
# Boxplot
ggplot(training, aes(x = 1, y = TARGET_WINS)) + 
  stat_boxplot(geom ='errorbar') + geom_boxplot() + 
  xlab("Boxplot") + ylab("") + theme(axis.text.x=element_blank(), axis.ticks.x=element_blank())
# Density plot
ggplot(training, aes(x = TARGET_WINS)) + 
  geom_histogram(aes(y=..density..), colour="black", fill="white",bins=50) +
  geom_density(alpha=.2, fill="#FF6666") + ylab("") + xlab("Density Plot with Mean") +
  geom_vline(aes(xintercept=mean(TARGET_WINS, na.rm=TRUE)), color="red", linetype="dashed", size=1)


clean_data <- function(df){
  # Change 0's to NA so they too can be imputed
  df <- df %>%
  mutate(TEAM_BATTING_SO = ifelse(TEAM_BATTING_SO == 0, NA, TEAM_BATTING_SO))
  # Remove the high pitching strikeout values
  df[which(df$TEAM_PITCHING_SO > 5346),"TEAM_PITCHING_SO"] <- NA
  # Drop the hit by pitcher variable
  df %>%
    select(-TEAM_BATTING_HBP)
}
training <- clean_data(training)


set.seed(42)
knn <- training %>% knnImputation()
apply_func <- function(df, knn){
  impute_me <- is.na(df$TEAM_BATTING_SO)
  df[impute_me,"TEAM_BATTING_SO"] <- knn[impute_me,"TEAM_BATTING_SO"] 
  impute_me <- is.na(df$TEAM_BASERUN_SB)
  df[impute_me,"TEAM_BASERUN_SB"] <- knn[impute_me,"TEAM_BASERUN_SB"] 
  impute_me <- is.na(df$TEAM_BASERUN_CS)
  df[impute_me,"TEAM_BASERUN_CS"] <- knn[impute_me,"TEAM_BASERUN_CS"] 
  impute_me <- is.na(df$TEAM_PITCHING_SO)
  df[impute_me,"TEAM_PITCHING_SO"] <- knn[impute_me,"TEAM_PITCHING_SO"]
  impute_me <- is.na(df$TEAM_FIELDING_DP)
  df[impute_me,"TEAM_FIELDING_DP"] <- knn[impute_me,"TEAM_FIELDING_DP"]
  return(df)
}
training <- apply_func(training, knn)



add_features <- function(df){
  df %>%
    mutate(TEAM_BATTING_1B = TEAM_BATTING_H - TEAM_BATTING_2B - TEAM_BATTING_3B - TEAM_BATTING_HR)
}
training <- add_features(training)


Here's what the data look like after imputation and correction:


training %>%
  gather(variable, value) %>%
  ggplot(., aes(value)) + 
  geom_density(fill = "indianred4", color="indianred4") + 
  facet_wrap(~variable, scales ="free", ncol = 4) +
  labs(x = element_blank(), y = element_blank())

quick_summary <- function(df){
  df %>%
    summary() 
}
quick_summary(training)

set.seed(42)
train_index <- createDataPartition(training$TARGET_WINS, p = .7, list = FALSE, times = 1)
train <- training[train_index,]
test <- training[-train_index,]

m1 <- lm(TARGET_WINS ~ TEAM_BATTING_H + TEAM_BATTING_BB + TEAM_FIELDING_E, data=train)
summary(m1)



m2 <- lm(TARGET_WINS ~ TEAM_BATTING_1B + TEAM_BATTING_2B + TEAM_BATTING_3B + TEAM_BATTING_HR + 
           TEAM_BATTING_BB + TEAM_FIELDING_E, data=train)
summary(m2)


A
full_formula <- "TARGET_WINS ~ TEAM_BATTING_2B + TEAM_BATTING_3B + TEAM_BATTING_HR + TEAM_BATTING_BB + TEAM_BATTING_SO + TEAM_BASERUN_SB + TEAM_BASERUN_CS + TEAM_PITCHING_H + TEAM_PITCHING_HR + TEAM_PITCHING_BB + TEAM_PITCHING_SO + TEAM_FIELDING_E + TEAM_FIELDING_DP + TEAM_BATTING_1B + I(TEAM_BATTING_2B^2) + I(TEAM_BATTING_3B^2) + I(TEAM_BATTING_HR^2) + I(TEAM_BATTING_BB^2) + I(TEAM_BATTING_SO^2) + I(TEAM_BASERUN_SB^2) + I(TEAM_BASERUN_CS^2) + I(TEAM_PITCHING_H^2) + I(TEAM_PITCHING_HR^2) + I(TEAM_PITCHING_BB^2) + I(TEAM_PITCHING_SO^2) + I(TEAM_FIELDING_E^2) + I(TEAM_FIELDING_DP^2) + I(TEAM_BATTING_1B^2) "
full_model <- lm(full_formula, train)
step_back <- MASS::stepAIC(full_model, direction="backward", trace = F)
poly_call <- summary(step_back)$call
m3 <- lm(poly_call[2], train)
summary(m3)

# Create log fielding error
train$TEAM_FIELDING_E_LOG <- log(train$TEAM_FIELDING_E)
m4 <- lm(TARGET_WINS ~ TEAM_BATTING_SO  + TEAM_BATTING_3B + 
           TEAM_BATTING_HR + TEAM_BASERUN_SB  + 
           TEAM_FIELDING_E_LOG*TEAM_PITCHING_H, data=train)
summary(m4)

test$m1 <- predict(m1, test)
test <- test %>%
  mutate(m1_error = TARGET_WINS - m1)
ggplot(test, aes(m1_error)) +
  geom_histogram(bins = 50) +
  annotate("text",x=0,y=10,
           label = paste("RMSE = ",
                         round(sqrt(mean(test$m1_error^2)),2)
                         ),
           color="white"
           )
summary(test$m1_error)

test$m2 <- predict(m2, test)
test <- test %>%
  mutate(m2_error = TARGET_WINS - m2)
ggplot(test, aes(m2_error)) +
  geom_histogram(bins = 50) +
  annotate("text",x=0,y=10,
           label = paste("RMSE = ",
                         round(sqrt(mean(test$m2_error^2)),2)
                         ),
           color="white"
           )
summary(test$m2_error)

test$m3 <- predict(m3, test)
test <- test %>%
  mutate(m3_error = TARGET_WINS - m3)
ggplot(test, aes(m3_error)) +
  geom_histogram(bins = 50) +
  annotate("text",x=0,y=10,
           label = paste("RMSE = ",
                         round(sqrt(mean(test$m3_error^2)),2)
                         ),
           color="white"
           )
summary(test$m3_error)

# Create log fielding error
test$TEAM_FIELDING_E_LOG <- log(test$TEAM_FIELDING_E)
test$m4 <- predict(m4, test)
test <- test %>%
  mutate(m4_error = TARGET_WINS - m4)
ggplot(test, aes(m4_error)) +
  geom_histogram(bins = 50) +
  annotate("text",x=0,y=10,
           label = paste("RMSE = ",
                         round(sqrt(mean(test$m4_error^2)),2)
                         ),
           color="white"
           )
summary(test$m4_error)

AIC(m1, m2, m3, m4)
summary(m3)




plot(m3$residuals, ylab="Residuals")
abline(h=0)

clean_data <- function(df){
  # Change 0's to NA so they too can be imputed
  df <- df %>%
  mutate(TEAM_BATTING_SO = ifelse(TEAM_BATTING_SO == 0, NA, TEAM_BATTING_SO))
  # Remove the high pitching strikeout values
  df[which(df$TEAM_PITCHING_SO > 5346),"TEAM_PITCHING_SO"] <- NA
  # Drop the hit by pitcher variable
  df %>%
    select(-TEAM_BATTING_HBP)
}
evaluation <- clean_data(evaluation)

set.seed(42)
knn <- evaluation %>% knnImputation()
apply_func <- function(df, knn){
  impute_me <- is.na(df$TEAM_BATTING_SO)
  df[impute_me,"TEAM_BATTING_SO"] <- knn[impute_me,"TEAM_BATTING_SO"] 
  impute_me <- is.na(df$TEAM_BASERUN_SB)
  df[impute_me,"TEAM_BASERUN_SB"] <- knn[impute_me,"TEAM_BASERUN_SB"] 
  impute_me <- is.na(df$TEAM_BASERUN_CS)
  df[impute_me,"TEAM_BASERUN_CS"] <- knn[impute_me,"TEAM_BASERUN_CS"] 
  impute_me <- is.na(df$TEAM_PITCHING_SO)
  df[impute_me,"TEAM_PITCHING_SO"] <- knn[impute_me,"TEAM_PITCHING_SO"]
  impute_me <- is.na(df$TEAM_FIELDING_DP)
  df[impute_me,"TEAM_FIELDING_DP"] <- knn[impute_me,"TEAM_FIELDING_DP"]
  return(df)
}
evaluation <- apply_func(evaluation, knn)

add_features <- function(df){
  df %>%
    mutate(TEAM_BATTING_1B = TEAM_BATTING_H - TEAM_BATTING_2B - TEAM_BATTING_3B - TEAM_BATTING_HR)
}
evaluation <- add_features(evaluation)


evaluation$PREDICT_WIN <- predict(m3, newdata=evaluation, interval="confidence")
Forecast <- cbind(evaluation$INDEX, evaluation$PREDICT_WIN[, 1], evaluation$PREDICT_WIN[, 2], evaluation$PREDICT_WIN[, 3])
colnames(Forecast) <- c("Index", "Predicted Wins", "CI Lower", "CI Upper")
round(Forecast,0)
