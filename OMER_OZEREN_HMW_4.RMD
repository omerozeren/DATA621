---
title: "HMW 4- Data 621"
author: "OMER OZEREN"
output:
  html_document:
    highlight: tango
    theme: journal
    toc: yes
    toc_depth: 5
    toc_float: yes
---
```{r setup, include=FALSE}
library(knitr)
library(kableExtra)
library(ggplot2)
library(dplyr)
library(caTools)
library(pscl)
library(ROCR)
library(MASS)
library(caret)
library(car)
library(Metrics)
library(quantreg)
library(pander)
opts_chunk$set(echo = TRUE)
```

## Summary

The goal of this project is to build two models - one binary regression model to predict whether a vehicle will be involved in a crash and a linear regression model to predict possible payout. 

Binary model generated fairly good results. With the accuracy of close to 80% it is noticeably better than just randomly guessing the outcome. Car insurance business is an important, big and complex business, so this accuracy may not be high enough in real-world situation (where auto insurance companies almost definitely maintain significant data science groups). However, this result is reasonable for this report. 

Linear model proved highly problematic. The most significant predictor for payout amount is the Blue Book value. This is reasonable. However, with $R^2$ value of 0.005, this model barely explains any variance in the outcome variable. Adding other variables that should influence the payout amount, such as `CAR_AGE`, did not have a significant effect.

The model was created using only onservations for vehicles involved in a crash (`TARGET_FLAG`==1). It is possible to create a linear regression model on all data, without this limitation. $R^2$ is significantly improved. In some tests, $R^2$ was about 0.27. However, I believe this explains the fit for observations with \$0 payout (vehicles not involved in a crash) and does not help fit the actual payouts any better. 

Looking at the relationship between Blue Book value and payout amount, there is a high amount of variability. There are many observations with low value, but high payout (too many to discount as outliers). This leads me to believe that some critical data is missing in order to accurately predict the payout. Such variables as severity of crash, whether the vehicle was totaled or not, number of vehicles involved in the crash should have a significant influence on payout amount.

## Data Exploration

The data set includes 8,161 observations with 24 variables (excluding the target variable).

#### Summary of Variables

```{r echo=FALSE, warning=FALSE}
ins <- read.csv(url(paste0("https://raw.githubusercontent.com/omerozeren/DATA621/master/insurance_training_data.csv")),
                na.strings=c("","NA"))
ins$INCOME <- as.numeric(gsub('[$,]', '', ins$INCOME)) # Convert income from Factor to Numeric
ins$HOME_VAL <- as.numeric(gsub('[$,]', '', ins$HOME_VAL)) # Convert home value from Factor to Numeric
levels(ins$MSTATUS)[match("z_No",levels(ins$MSTATUS))] <- "No" # Remove 'z_'
levels(ins$SEX)[match("z_F",levels(ins$SEX))] <- "F" # Remove 'z_'
levels(ins$EDUCATION)[match("z_High School",levels(ins$EDUCATION))] <- "High School" # Remove 'z_'
ins$EDUCATION <- factor(ins$EDUCATION,levels(ins$EDUCATION)[c(1,5,2:4)]) # Reorder levels
levels(ins$JOB)[match("z_Blue Collar",levels(ins$JOB))] <- "Blue Collar" # Remove 'z_'
ins$BLUEBOOK <- as.numeric(gsub('[$,]', '', ins$BLUEBOOK)) # Convert value from Factor to Numeric
levels(ins$CAR_TYPE)[match("z_SUV",levels(ins$CAR_TYPE))] <- "SUV" # Remove 'z_'
levels(ins$RED_CAR)[match("no",levels(ins$RED_CAR))] <- "No"
levels(ins$RED_CAR)[match("yes",levels(ins$RED_CAR))] <- "Yes"
ins$OLDCLAIM <- as.numeric(gsub('[$,]', '', ins$OLDCLAIM)) # Convert from Factor to Numeric
levels(ins$URBANICITY)[match("Highly Urban/ Urban",levels(ins$URBANICITY))] <- "Urban"
levels(ins$URBANICITY)[match("z_Highly Rural/ Rural",levels(ins$URBANICITY))] <- "Rural"
ins <- ins[-c(1)]
ins$JOB <- factor(ins$JOB,levels(ins$JOB)[c(7, 8, 3, 1, 6, 5, 4, 2)]) # Reorder levels
vars <- data.frame(Variable = c("KIDSDRIV",
                                "AGE",
                                "HOMEKIDS",
                                "YOJ",
                                "INCOME",
                                "PARENT1",
                                "HOME_VAL",
                                "MSTATUS",
                                "SEX",
                                "EDUCATION",
                                "JOB",
                                "TRAVTIME",
                                "CAR_USE",
                                "BLUEBOOK",
                                "TIF",
                                "CAR_TYPE",
                                "RED_CAR",
                                "OLDCLAIM",
                                "CLM_FREQ",
                                "REVOKED",
                                "MVR_PTS",
                                "CAR_AGE",
                                "URBANICITY"
                                ), 
                   Type = c("Integer", "Integer", "Integer", "Integer", "Numeric",
                            "Factor", "Numeric", "Factor", "Factor", "Factor",
                            "Factor", "Integer", "Factor", "Numeric", "Integer",
                            "Factor", "Factor", "Numeric", "Integer", "Factor", 
                            "Integer", "Integer", "Factor"
                            ), 
                   Description = c("No of children driving.",
                                   "Age of driver.",
                                   "No of children at home.",
                                   "Years on the job.",
                                   "Income.",
                                   "Single parent flag.",
                                   "Home value.",
                                   "Married flag.",
                                   "Gender.",
                                   "Maximum education level.",
                                   "Job category.",
                                   "Distance to work.",
                                   "Vehicle use.",
                                   "Vehicle value.",
                                   "Time in force.",
                                   "Vehicle type.",
                                   "Red car flag.",
                                   "Total payout of claims.",
                                   "No of claims (past 5 years).",
                                   "Revoked license flag.",
                                   "Motor vehicle record points.",
                                   "Vehicle age.",
                                   "Home/work area."
                                   ), 
                   Comments = c("Ranges from 0 to 4.",
                                "Ranges from 16 to 81. Contains 6 NAs.",
                                "Ranges from 0 to 5.",
                                paste0("Ranges from 0 to 23. ",
                                       "Contains 454 NAs (about 5.56% of all observations)."),
                                paste0("Ranges from $0 to $367,000. ",
                                       "Contains 445 NAs (about 5.45% of all observations). ",
                                       "Was converted to numeric by removing ",
                                       "dollar signs and commas."),
                                "Values: No, Yes.",
                                paste0("Ranges from $0 to $885,000. ",
                                       "Contains 464 NAs (about 5.69% of all observations). ",
                                       "Was converted to numeric by removing ",
                                       "dollar signs and commas."),
                                "Values: Yes, No.",
                                "Values: M, F.",
                                "Values: <High School, High School, Bachelors, Masters, PhD.",
                                paste0("Values: [Blank], Clerical, Doctor, Home Maker, Lawyer,",
                                       " Manager, Professional, Student, Blue Collar."),
                                "Ranges from 5 to 142.",
                                "Values: Commercial, Private.",
                                paste0("Ranges from $1,500 to $69,740. Was converted to ",
                                       "numeric by removing dollar signs and commas."),
                                "Ranges from 1 to 25.",
                                "Values: Minivan, Panel Truck, Pickup, Sports Car, Van, SUV.",
                                "Values: No, Yes",
                                paste0("Ranges from $0 to $57,040. Was converted to numeric ",
                                       "by removing dollar signs and commas."),
                                "Ranges from 0 to 5.",
                                "Values: No, Yes.",
                                "Ranges from 0 to 13.",
                                paste0("Ranges from -3 to 28. Contains 510 NAs (about ",
                                       "6.25% of all observations)."),
                                "Values: Urban, Rural."
                                )
                   )
pander(vars, justify="left", split.cell=20)
```

The table below shows summary of numeric variables. 

```{r echo=FALSE, warning=FALSE}
sumIns = data.frame(Variable = character(),
                    Min = integer(),
                    Median = integer(),
                    Mean = double(),
                    SD = double(),
                    Max = integer(),
                    Num_NAs = integer(),
                    Num_Zeros = integer())
for (i in c(3:7,9,14,16,17,20,21,23,24)) {
  sumIns <- rbind(sumIns, data.frame(Variable = colnames(ins)[i],
                                     Min = min(ins[,i], na.rm=TRUE),
                                     Median = median(ins[,i], na.rm=TRUE),
                                     Mean = mean(ins[,i], na.rm=TRUE),
                                     SD = sd(ins[,i], na.rm=TRUE),
                                     Max = max(ins[,i], na.rm=TRUE),
                                     Num_NAs = sum(is.na(ins[,i])),
                                     Num_Zeros = length(which(ins[,i]==0)))
  )
}
colnames(sumIns) <- c("Variable", "Min", "Median", "Mean", "SD", "Max", "Num of NAs", "Num of Zeros")
pander(sumIns)
```

In the table above we can see a significant number of observations with value of 0. There is a logical explanation that these observations are valid:

- `KIDSDRIV` and `HOMEKIDS`: households without children
- `YOJ` and `INCOME`: unemployed individuals
- `HOME_VAL`: renters
- `OLDCLAIM`, `CLM_FREQ` and `MVR_PTS`: safe drivers with no claims or DMV points

#### Hangling _NAs_

Several variables - `AGE`, `YOJ`, `INCOME`, `HOME_VAL`, `CAR_AGE` - contained some `NA` values. The number seemed significant; however, given the large number of observations it was safer to remove incomplete cases. 

Additionally, `JOB` contained some blank values. It is possible that the blank value represented a certain category, for example _Unemployed_; however, without any indication whether it is missing value or specific category, it was treated similarly to `NA` values.

Additionally, `CAR_AGE` had 4 values that appeared to be errors. One observations had `CAR_AGE` as -3 and three observations had `CAR_AGE` as 0. The negative value is clearly wrong; however, it is possible to make a case that 0 is a valid value (new car). Given low number of 0 values and high number of 1 for that variable, it is likely that 0 is an error and 1 represents a new car. The 4 affected records have were removed.

```{r echo=FALSE, warning=FALSE}
ins <- ins[complete.cases(ins), ]
ins[ins$CAR_AGE<1,'CAR_AGE'] <- NA
ins <- ins[complete.cases(ins), ]
```

After these steps 2,120 observations were removed leaving 6,041 observations. As stated above, although the number of remove observations is significant, there are enough observations left to perform the analysis. For categorical variables, there are enough examples for each category.

#### Additional Details

- Boxplots and histograms were inspected for all variable in order to see any anomalies.
- Multiple values were prefixed with text 'z_', which was removed from all observations. Affected variables are `MSTATUS`, `EDUCATION`, `JOB`, `CAR_TYPE` and `URBANICITY`.
- Index column present in the data set has been removed. 
- Levels for `EDUCATION` have been re-ordered to follow the most common order: _<High School_, _High School_, _Bachelors_, _Masters_, and _PhD_.
- Levels for `JOB` have been re-ordered to follow general order from low-paying to high-paying occupations: _Student_, _Blue Collar_, _Home Maker_, _Clerical_, _Professional_, _Manager_, _Lawyer_, and _Doctor_.
- Counts of observations for possible values of `PARENT1`:
```{r echo=FALSE, warning=FALSE}
pander(table(ins$PARENT1))
```
- Counts of observations for possible values of `MSTATUS`:
```{r echo=FALSE, warning=FALSE}
pander(table(ins$MSTATUS))
```
- Counts of observations for possible values of `SEX`:
```{r echo=FALSE, warning=FALSE}
pander(table(ins$SEX))
```
- Counts of observations for possible values of `EDUCATION`:
```{r echo=FALSE, warning=FALSE}
pander(table(ins$EDUCATION))
```
- Counts of observations for possible values of `JOB`:
```{r echo=FALSE, warning=FALSE}
pander(table(ins$JOB), split.cell=8)
```
- Counts of observations for possible values of `CAR_USE`:
```{r echo=FALSE, warning=FALSE}
pander(table(ins$CAR_USE))
```
- Counts of observations for possible values of `CAR_TYPE`:
```{r echo=FALSE, warning=FALSE}
pander(table(ins$CAR_TYPE))
```
- Counts of observations for possible values of `RED_CAR`:
```{r echo=FALSE, warning=FALSE}
pander(table(ins$RED_CAR))
```
- Counts of observations for possible values of `REVOKED`:
```{r echo=FALSE, warning=FALSE}
pander(table(ins$REVOKED))
```
- Counts of observations for possible values of `URBANICITY`:
```{r echo=FALSE, warning=FALSE}
pander(table(ins$URBANICITY))
```
- Counts of observations for possible values of `KIDSDRIV`:
```{r echo=FALSE, warning=FALSE}
pander(table(ins$KIDSDRIV))
```
- Counts of observations for possible values of `HOMEKIDS`
```{r echo=FALSE, warning=FALSE}
pander(table(ins$HOMEKIDS))
```
- Counts of observations for possible values of `CLM_FREQ`
```{r echo=FALSE, warning=FALSE}
pander(table(ins$CLM_FREQ))
```
- As shown above there are several values for `KIDSDRIV`, `HOMEKIDS` and `CLM_FREQ` with few observations. Specifically, there are only 2 observations with 4 children driving and only 11 obserations with 5 children at home or with 5 claims within the past 5 years. It is important to note that these three variables are treated as numeric rather than categorical.

#### Target/Dependent Variable `TARGET_FLAG`

Target variable for the binary regression model represents a flag whether a vehicle was involved in a crash. There are 4,440 observation with `TARGET_FLAG` value of 0 and 1,601 observations with `TARGET_FLAG` value of 1 making it about 75/25 split, or more precisely there are **73.5% of 0s and 26.5% of 1s**.

#### Target/Dependent Variable `TARGET_AMT`

Target variable for the linear regression model represents the cost if a vehicle was involved in a crash. The value is presented only for observations with `TARGET_FLAG` of 1. Distribution of `TARGET_AMT` has a long right tail. There are no missing values.

```{r echo=FALSE, warning=FALSE}
amt <- data.frame(Min = min(ins[ins$TARGET_FLAG==1, 'TARGET_AMT'], na.rm=TRUE),
                  Median = median(ins[ins$TARGET_FLAG==1, 'TARGET_AMT'], na.rm=TRUE),
                  Mean = mean(ins[ins$TARGET_FLAG==1, 'TARGET_AMT'], na.rm=TRUE),
                  SD = sd(ins[ins$TARGET_FLAG==1, 'TARGET_AMT'], na.rm=TRUE),
                  Max = max(ins[ins$TARGET_FLAG==1, 'TARGET_AMT'], na.rm=TRUE),
                  Num_NAs = sum(is.na(ins[ins$TARGET_FLAG==1, 'TARGET_AMT'])),
                  Num_Zeros = length(which(ins[ins$TARGET_FLAG==1, 'TARGET_AMT']==0)))
pander(amt)
ggplot(data=ins[ins$TARGET_FLAG==1,], aes(x = TARGET_AMT)) + 
  geom_histogram(aes(y=..density..), colour="black", fill="white", binwidth = 1000) +
  geom_density(alpha=.2, fill="#FF6666") + ylab("") + xlab("Density Plot with Mean") +
  geom_vline(aes(xintercept=mean(ins[ins$TARGET_FLAG==1,'TARGET_AMT'])), 
             color="red", linetype="dashed", size=1)
```

## Modelling: Generalized Linear Model

The first model will be used to predict whether a vehicle will be involved in a crash. The dependent variable, `TARGET_FLAG`, is binary. For this project it is assumed that observations are independent of each other as there is no reason to believe otherwise.

To test the accuracy of the model the main data set is split into training and testing data sets. The training set includes 75% of randomly chosen observations (4,531) while the testing set includes remaining 25% (1,510).

```{r echo=FALSE, warning=FALSE}
set.seed(88)
split <- sample.split(ins$TARGET_FLAG, SplitRatio = 0.75)
insTRAIN <- subset(ins, split == TRUE)
insTEST <- subset(ins, split == FALSE)
```

The starting point is a model that includes all independent variables. It has AIC value of 4085.5 and accuracy of 79.47%. Summary is below.

```{r echo=FALSE, warning=FALSE}
model <- glm (TARGET_FLAG ~ .-TARGET_AMT, data = insTRAIN, family = binomial(link="logit"))
summary(model)
```

Running this model through the stepwise algorithm (`stepAIC` from the `MASS` package), removed `AGE`, `HOMEKIDS`, `YOJ`, `SEX`, `RED_CAR` and `CAR_AGE`. AIC is reduced slightly to 4078.2 and accuracy is improved very slightly to 79.54%.

`INCOME` and `HOME_VAL` are very right-skewed. To make results more normal, they are log-transformed (adding 1 to make sure that log-transformation is possible for 0 values). The new model again has slightly lower AIC of 4073.2 and slightly higher accuracy at 79.73%.

```{r echo=FALSE, warning=FALSE}
model <- glm(formula = TARGET_FLAG ~ KIDSDRIV + log(INCOME+1) + PARENT1 + log(HOME_VAL+1) + 
               MSTATUS + EDUCATION + JOB + TRAVTIME + CAR_USE + BLUEBOOK + 
               TIF + CAR_TYPE + OLDCLAIM + CLM_FREQ + REVOKED + MVR_PTS + 
               URBANICITY, family = binomial(link = "logit"), data = insTRAIN)
summary(model)
```

Checking this model using various evaluation methods, a potential issue was discovered with variance-inflation (using `vif` in the `car` package). 

```{r echo=FALSE, warning=FALSE}
vif(model)
```

The value for `JOB` is very high (although when adjusted for degrees of freedom is does not appear to be problematic). The value for `EDUCATION` is also fairly high. Comparing these two variables we can see possible correlation between them. 

```{r echo=FALSE, warning=FALSE}
ggplot(data = ins, aes(JOB, EDUCATION)) +
  geom_jitter()
```

This is quite logical. Some occupations, such as doctor or lawyer, require a graduate level degree. At the same time lower levels of education are more connect to blue collar work rather than professional work. `JOB` was removed from the model.

#### Final Model

The formula for the final generalized linear model is `TARGET_FLAG ~ KIDSDRIV + log(INCOME+1) + PARENT1 + log(HOME_VAL+1) + MSTATUS + EDUCATION + TRAVTIME + CAR_USE + BLUEBOOK + TIF + CAR_TYPE + OLDCLAIM + CLM_FREQ + REVOKED + MVR_PTS + URBANICITY`. AIC is 4107.

```{r echo=FALSE, warning=FALSE}
model <- glm(formula = TARGET_FLAG ~ KIDSDRIV + log(INCOME+1) + PARENT1 + log(HOME_VAL+1) + 
               MSTATUS + EDUCATION + TRAVTIME + CAR_USE + BLUEBOOK + 
               TIF + CAR_TYPE + OLDCLAIM + CLM_FREQ + REVOKED + MVR_PTS + 
               URBANICITY, family = binomial(link = "logit"), data = insTRAIN)
summary(model)
pred <- predict(model, newdata=subset(insTEST, select=c(1:25)), type='response')
cm <- confusionMatrix(as.factor(insTEST$TARGET_FLAG), as.factor(ifelse(pred > 0.5,1,0)))
pander(cm$table)
#cm$overall['Accuracy']
pander(pR2(model)) # McFadden R^2
```

Accuracy is 79.80% which is noticeably higher than a flip of a coin. McFadden $R^2$ is 0.2254. Area under the curve (see plot below) is 0.8022.

```{r echo=FALSE, warning=FALSE}
pr <- prediction(pred, insTEST$TARGET_FLAG)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf, colorize = TRUE, text.adj = c(-0.2,1.7))
auc <- performance(pr, measure = "auc")
```

Results below are for K-fold cross validation using 10 iterations. They hold up against results observed with simple training.

```{r echo=FALSE, warning=FALSE}
ctrl <- trainControl(method = "repeatedcv", number = 10, savePredictions = TRUE)
model_fit <- train(TARGET_FLAG ~ KIDSDRIV + log(INCOME+1) + PARENT1 + log(HOME_VAL+1) + 
                     MSTATUS + EDUCATION + TRAVTIME + CAR_USE + BLUEBOOK + 
                     TIF + CAR_TYPE + OLDCLAIM + CLM_FREQ + REVOKED + MVR_PTS + 
                     URBANICITY,  data=insTRAIN, method="glm", family="binomial",
                   trControl = ctrl, tuneLength = 5)
pred <- predict(model_fit, newdata=insTEST)
confusionMatrix(as.factor(insTEST$TARGET_FLAG), as.factor(ifelse(pred > 0.5,1,0)))
```

Analyzing deviance (using `anova`) shows that all terms are significant although significance for `MSTATUS` and `TRAVTIME` are less significant than all other terms.

```{r echo=FALSE, warning=FALSE}
anova(model, test="Chisq")
```

Variance inflation factors are low for all variables, so correlation is not high. 

```{r echo=FALSE, warning=FALSE}
vif(model)
```

#### Coefficient Analysis 

Looking at the model coefficients, the following variables make the crash more likely:

- Higher number of children driving
- Single parents and unmarried individuals
- Higher commuting distance
- License revocation within the past 7 years
- Higher number of claims within the past 5 years
- Higher number of DMV points

Additional points: 

- Commercial vehicle use is more likely to result in a crash than private. 
- Rural environment is less likely to result in a crash than urban. 
- Individuals who have been customers longer are less likely to have a crash.
- Higher Blue Book value makes it less likely to result in a crash.
- Education makes it less likely to result in a crash.
- Higher income and home value make it less likely to result in a crash.
- Car type is hard to evaluate, but generally various types make it more likely to have a crash (to different degrees).
- Interestingly, larger previous payout make it less likely to result in a crash. 

Most of these findings are in line with theoretical effect. Some coefficients help clarify it. 

## Modelling: Linear Model

Linear modelling is used to predict the amount of the payout in case of a crash (`TARGET_AMT`). **Only observations where a crash has occurred (`TARGET_FLAG`==1) are used in training the linear model.** If there is no crash, payout will not be needed. As such if observations without a crash are included in the model, they may skew the results.

Isolating all observations with a reported crash, leaves 1,601 observations. 

Again the data is divided into a training set (75%; 1,200 observations) and a testing set (25%; 401 observations).

```{r echo=FALSE, warning=FALSE}
insLM <- ins[ins$TARGET_FLAG==1,]
split <- sample.split(insLM$TARGET_AMT, SplitRatio = 0.75)
insLMtrain <- subset(insLM, split == TRUE)
insLMtest <- subset(insLM, split == FALSE)
```

The following 5 models were built and compared. 

- **Model 1**: All independent variables.
- **Model 2**: Model 1 optimized using stepwise algorithm. This dropped all variables except `PARENT1`, `MSTATUS`, `BLUEBOOK`, `OLDCLAIM`, `CLM_FREQ`, `REVOKED` and `JOB`.
- **Model 3**: Some variables from model 2 dropping less signficant ones. This model includes `BLUEBOOK`, `OLDCLAIM`, `CLM_FREQ` and `REVOKED`.
- **Model 4**: Variables that theoretically should have an effect: `BLUEBOOK`, `CAR_AGE`, `CAR_TYPE`.
- **Model 5**: Only `BLUEBOOK`. This variable seems the most significant.

```{r echo=FALSE, warning=FALSE}
lmModels <- data.frame(Model = c("Model 1","Model 2","Model 3","Model 4","Model 5"), 
                       R2 = c(0.01808, 0.02218, 0.01366, 0.01351, 0.01202), 
                       RMSE = c(8416.4, 8367.4, 8343.9, 8372.7, 8336.2))
colnames(lmModels) <- c("Model", "Adjusted R^2", "Root-Mean Square Error")
pander(lmModels)
```

Even though Model 5 has the worst adjusted $R^2$, it has the best RMSE value (accuracy in prediction using the testing set). It is also the simplest model using only Blue Book value. Further analysis will be based on this model.

```{r echo=FALSE, warning=FALSE}
lmModel <- lm(TARGET_AMT ~ BLUEBOOK,data = insLMtrain)
summary(lmModel)
```

Looking at the Q-Q plot, it is clear that there is a significant number of outliers at the higher range. This looks very problematic.

```{r echo=FALSE, warning=FALSE}
qqnorm(lmModel$residuals)
qqline(lmModel$residuals)
```

Consider Box-Cox transformation (plot below is generated using `boxcox` from the `MASS` package). 

```{r echo=FALSE, warning=FALSE}
boxcox(lmModel)
```

if lambda is picked to be 0, then target variable should be log-transformed. 

```{r echo=FALSE, warning=FALSE}
lmModel <- lm(log(TARGET_AMT) ~ BLUEBOOK,data = insLMtrain)
summary(lmModel)
```

This drastically reduced already low adjusted $R^2$. However, Q-Q plot is improved. It is still not without issues, but noticeably better. Additionally, looking at the residual plots, there seems to be independence of observations. Another problem area is constant variance when plotting fitted values vs residuals. 

```{r echo=FALSE, warning=FALSE}
qqnorm(lmModel$residuals)
qqline(lmModel$residuals)
plot(lmModel$residuals, ylab="Residuals")
abline(h=0)
plot(lmModel$fitted.values, lmModel$residuals, xlab="Fitted Values", ylab="Residuals")
abline(h=0)
```

#### Robust/Quantile Regression

Looking at the scatterplot of `BLUEBOOK` vs `log(TARGET_AMT)`, there is a lot of variance and a lot of outliers. It is possible that some points are leverage points that interfere with the model. Two additional model were built in an effort to account for that.

The first model was created using robust regression (`rlm` in the `MASS` package). The second model was created using  quantile regression (`rq` in the `quantreg` package). 

Original model has RMSE of 0.7815. RLM model very slightly improves it to 0.7811. Finally, RQ model also very slightly improves it to 0.7771. Fits for all three models are presented in the scatterplot below.

```{r echo=FALSE, warning=FALSE}
lmModel2 <- rlm(log(TARGET_AMT) ~ BLUEBOOK, data = insLMtrain)
lmModel3 <- rq(log(TARGET_AMT) ~ BLUEBOOK, data = insLMtrain)
plot(log(TARGET_AMT) ~ BLUEBOOK, data = insLMtrain)
abline(lmModel)
abline(lmModel2, col="red")
abline(lmModel3, col="blue")
legend("topright", inset=0.05, bty="n",
       legend=c("lm fit", "rlm fit", "rq fit"),
       lty=c(1,1,1),
       col=c("black", "red", "blue"))
```

Variation between model is small, so this approach did not generate significant improvement. Please see the Summary section in the beginning of the report for more analysis of linear model results.

## Evaluation Data Set

For illustration purposes, this report only includes the first 100 entries in the `insurance-evaluation-data.csv` file.

Evalution data is missing some `INCOME` and `HOME_EVAL` values that are used in the binary regression model. Rather than trying to impute those values by replacing them with 0 or mean or media values or by building a model to predict them, these values are left as they are and the model cannot be used to predict the outcome for corresponding observations. Approach to imputing these values may change based on the situation and for this report, this part was not assumed. If any of these variables are likely to be missing in real-world use it may be worthwhile investigating omitting these variables from the model altogether.

```{r echo=FALSE, warning=FALSE}
eval <- read.csv(url(paste0("https://raw.githubusercontent.com/omerozeren/DATA621/master/insurance-evaluation-data.csv")),
                 na.strings=c("","NA"))
results <- eval[,1]
eval$INCOME <- as.numeric(gsub('[$,]', '', eval$INCOME)) # Convert income from Factor to Numeric
eval$HOME_VAL <- as.numeric(gsub('[$,]', '', eval$HOME_VAL)) # Convert home value from Factor to Numeric
levels(eval$MSTATUS)[match("z_No",levels(eval$MSTATUS))] <- "No" # Remove 'z_'
levels(eval$SEX)[match("z_F",levels(eval$SEX))] <- "F" # Remove 'z_'
levels(eval$EDUCATION)[match("z_High School",levels(eval$EDUCATION))] <- "High School" # Remove 'z_'
eval$EDUCATION <- factor(eval$EDUCATION,levels(eval$EDUCATION)[c(1,5,2:4)]) # Reorder levels
levels(eval$JOB)[match("z_Blue Collar",levels(eval$JOB))] <- "Blue Collar" # Remove 'z_'
eval$BLUEBOOK <- as.numeric(gsub('[$,]', '', eval$BLUEBOOK)) # Convert value from Factor to Numeric
levels(eval$CAR_TYPE)[match("z_SUV",levels(eval$CAR_TYPE))] <- "SUV" # Remove 'z_'
levels(eval$RED_CAR)[match("no",levels(eval$RED_CAR))] <- "No"
levels(eval$RED_CAR)[match("yes",levels(eval$RED_CAR))] <- "Yes"
eval$OLDCLAIM <- as.numeric(gsub('[$,]', '', eval$OLDCLAIM)) # Convert from Factor to Numeric
levels(eval$URBANICITY)[match("Highly Urban/ Urban",levels(eval$URBANICITY))] <- "Urban"
levels(eval$URBANICITY)[match("z_Highly Rural/ Rural",levels(eval$URBANICITY))] <- "Rural"
eval$JOB <- factor(eval$JOB,levels(eval$JOB)[c(7, 8, 3, 1, 6, 5, 4, 2)]) # Reorder levels
eval <- eval[-c(1)]
pred <- predict(model, newdata=eval, type="response")
results <- cbind(results, prob=round(pred,4))
results <- cbind(results, predict=round(pred,0))
pred <- predict(lmModel, newdata=eval, type="response")
results <- cbind(results, exp(pred))
results <- as.data.frame(results)
results[results$predict==0 & !is.na(results$predict),'V4'] <- NA
results[is.na(results$predict),'V4'] <- NA
colnames(results) <- c("Index", "TARGET_FLAG Prob.", "TARGET_FLAG", "TARGET_AMT")
pander(head(results, 100))
```

