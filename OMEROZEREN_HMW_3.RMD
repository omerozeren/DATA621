title: "HMW 3- Data 621"
author: "OMER OZEREN"
output:
  html_document:
    highlight: tango
    theme: journal
    toc: yes
    toc_depth: 5
    toc_float: yes
---

```{r setup, include=FALSE}
library(knitr)
library(kableExtra)
library(ggplot2)
library(dplyr)
library(caTools)
library(pscl)
library(ROCR)
library(MASS)
library(caret)
library(car)
opts_chunk$set(echo = TRUE)
```

## Summary

This report covers an attempt to build a binary logistic regression model to predict whether the crime rate is above the median crime rate. The model is based on a data set containing information on crime for various Boston neighborhoods. 

## Data Exploration

The data set includes 466 observations with 12 variables (excluding the target variable).

#### Summary of Variables

```{r echo=FALSE, warning=FALSE}
crime <- read.csv(url(paste0("https://raw.githubusercontent.com/omerozeren/DATA621/master/crime-training-data_modified.csv")),
                na.strings=c("","NA"))
sumCrime = data.frame(Variable = character(),
                      Min = integer(),
                      Median = integer(),
                      Mean = double(),
                      SD = double(),
                      Max = integer(),
                      Num_NAs = integer(),
                      Num_Zeros = integer())
for (i in 1:13) {
  sumCrime <- rbind(sumCrime, data.frame(Variable = colnames(crime)[i],
                                   Min = min(crime[,i], na.rm=TRUE),
                                   Median = median(crime[,i], na.rm=TRUE),
                                   Mean = mean(crime[,i], na.rm=TRUE),
                                   SD = sd(crime[,i], na.rm=TRUE),
                                   Max = max(crime[,i], na.rm=TRUE),
                                   Num_NAs = sum(is.na(crime[,i])),
                                   Num_Zeros = length(which(crime[,i]==0)))
                 )
}
colnames(sumCrime) <- c("Variable", "Min", "Median", "Mean", "SD", "Max", "Num of NAs", "Num of Zeros")
kable(sumCrime, row.names=FALSE)
```

#### Independent Variables

- `zn` - _proportion of residential land zoned for large lots (over 25,000 square feet)_ - 339 out of 466 (or about 76%) of observations have a value of 0. It is possible that majority of neighborhoods will not have any residential land zoned for large lots. Therefore, it is likely that 0 represents a valid value rather than a missing one. 
- `indus` - _proportion of non-retail business acres per suburb_
- `chas` - _a dummy variable for whether the suburb borders the Charles River (1) or not (0)_ - 433 out of 466 (or about 92.9%) of observations have a value of 0. Even though the Charles River is a promimnent feature of the Boston area, it is quite reasonable to assume that most neighborhoods do not border the river.
- `nox` - _nitrogen oxides concentration (parts per 10 million)_ - Looking at the scatterplot there seems to be some correlation between the nitrogen oxides concentration and the target variable.

```{r echo=FALSE, warning=FALSE}
ggplot(crime, aes(x=nox, y=target)) + 
  geom_point() +
  stat_smooth(method="glm", method.args=list(family="binomial"), se=FALSE)
```

- `rm` - _average number of rooms per dwelling_ - Because this is an average of number of rooms, this is a continous variable.
- `age` - _proportion of owner-occupied units built prior to 1940_ - There is nothing unusual about this variable; however, it is interesting to note that the mean of 68.37 and median of 77.15 shows that there is a large number of pre-war buildings. Not surprising for an old city like Boston.
- `dis` - _weighted mean of distances to five Boston employment centers_ - Majority of observations above the median crime rate are within 5 miles of an employment center (there are only 2 observations over 5 miles away). And there may be some correlation between distance and the target variable.
- `rad` - _index of accessibility to radial highways_ - This is a discrete variable with 9 different values in the observations (1 through 8 and 24). The smallest bucket is `rad` value of 7 with 15 observations.
- `tax` - _full-value property-tax rate per $10,000_
- `ptratio` - _pupil-teacher ratio by town_
- `lstat` - _lower status of the population (percent)_
- `medv` - _median value of owner-occupied homes in \$1000s_

#### Target/Dependent Variable

- `target` - _whether the crime rate is above the median crime rate (1) or not (0)_ - There are 237 observation with `target` value of 0 and 229 observations with `target` value of 1 making it about 50/50 split, or more precisely there are **50.86% of 0s and 49.14% of 1s**.

#### Correlation Matrix

Below is the correlation matrix for the data set. 

- There is a very high correlation (0.91) between `tax` and `rad`. Meaning behind `rad` values/categories is not explicitly specified. It may be that the higher the highway accessibility is, the higher property taxes are. Alternatively, radial highways and higher property taxes may signify suburbs while lack of radial highways may imply inner city (with possibly poorer, lower taxed properties).
- `nox` has the highest correlation with the target variable, but `age`, `dis`, `rad` and `tax` are also fairly highly correlated to `target` (above 0.6).
- The following pairs have correlation at or above 0.7 (or below -0.7 in case of negative correlation): `nox`/`indus`, `dis`/`indus`, `tax`/`indus`,  `age`/`nox`, `dis`/`nox`, `medv`/`rm`, `dis`/`age` and `medv`/`lstat`.

```{r echo=FALSE, warning=FALSE}
cm <- cor(crime, use="pairwise.complete.obs")
cm <- round(cm, 2)
cmout <- as.data.frame(cm) %>% mutate_all(function(x) {
  cell_spec(x, "latex", color = ifelse(x>0.69 | x<(-0.69),"blue","black"))
  })
rownames(cmout) <- colnames(cmout)
cmout %>%
  kable("latex", escape = F, align = "c", row.names = TRUE) %>%
  kable_styling("striped", full_width = F)
```

#### Scatterplot Matrix

```{r echo=FALSE, warning=FALSE}
pairs(crime)
```

- Reviewing the scatterplot matrix shows several pairs with possible relationships. Two most prominent are `nox`/`dis` and `lstat`/`medv`. 
- `rm` and `medv` may have a linear relationship as well. 
- `rad` and `tax` have a very prominent outlier. Further inspection of data shows that all observations with `rad` value of 24 have a `tax` value of 666, so the outlier is actually multiple observations mapped to the same spot. Interestingly, all of these obervations also have a `target` value of 1. This combination may warrant closer inspection.

Above data analysis treats `rad` and `chas` as numeric variables; however, treating them as categorical variables may better reflect the nature of those variables, so for modelling they will be converted to factors. 

```{r echo=FALSE, warning=FALSE}
crime[,'rad'] <- as.factor(crime[,'rad'])
crime[,'chas'] <- as.factor(crime[,'chas'])
```

## Modelling

The dependent variable, `target`, is binary. For this project it is assumed that observations are independent of each other as there is no reason to believe otherwise.

As the first step, in order to test and compare performance of various models, data was split into training (75%) and testing (25%) sets. The training set includes 350 randomly chosen observations and the testing set includes 116 remaining observations. 

```{r echo=FALSE, warning=FALSE}
set.seed(88)
split <- sample.split(crime$target, SplitRatio = 0.75)
crimeTRAIN <- subset(crime, split == TRUE)
crimeTEST <- subset(crime, split == FALSE)
```

#### Model 1: Variables with High Correlation to Target Variable

The first model includes 5 variables with the highest correlation coefficients when compared agains the target variable. This simple model will allow for testing methodology as well as corresponding R code.

```{r echo=FALSE, warning=FALSE}
model <- glm (target ~ nox+age+dis+rad+tax, data = crimeTRAIN, family = binomial(link="logit"))
summary(model)
pred <- predict(model, newdata=subset(crimeTEST, select=c(1:12)), type='response')
cm <- confusionMatrix(as.factor(crimeTEST$target), as.factor(ifelse(pred > 0.5,1,0)))
cm$table
accuracy <- cm$overall['Accuracy']
R2 <- pR2(model)["McFadden"]
```

Model summary and confusion matrix of running this model against test data are above. The accuracy rate (`r accuracy`) is very good and the McFadden R^2 value (`r R2`) is also high. AIC value is 123.66. Additionally, consider the ROC curve for this model.

```{r echo=FALSE, warning=FALSE}
pr <- prediction(pred, crimeTEST$target)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf, colorize = TRUE, text.adj = c(-0.2,1.7))
auc <- performance(pr, measure = "auc")@y.values[[1]]
```

Area under the curve is `r auc`.

#### Model 2: All Variables

The second model includes all 12 available independent variables.

```{r echo=FALSE, warning=FALSE}
model <- glm (target ~ ., data = crimeTRAIN, family = binomial(link="logit"))
summary(model)
pred <- predict(model, newdata=subset(crimeTEST, select=c(1:12)), type='response')
cm <- confusionMatrix(as.factor(crimeTEST$target), as.factor(ifelse(pred > 0.5,1,0)))
cm$table
accuracy <- cm$overall['Accuracy']
R2 <- pR2(model)["McFadden"]
```

Model summary and confusion matrix of running this model against test data are above. The accuracy rate (`r accuracy`) is very good and the McFadden R^2 value (`r R2`) is also high. AIC value is 128.1. Additionally, consider the ROC curve for this model.

```{r echo=FALSE, warning=FALSE}
pr <- prediction(pred, crimeTEST$target)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf, colorize = TRUE, text.adj = c(-0.2,1.7))
auc <- performance(pr, measure = "auc")@y.values[[1]]
```

Area under the curve is `r auc`. 

Comparing to the first model AIC has slightly increased (worse), but accuracy, McFadden R^2 and AUC all also slightly increased (better). 

#### Model 3: _StepAIC_ Method

The third model starts with all 12 available independent variables, but then drops them one by one using the stepwise algorithm.

```{r echo=FALSE, warning=FALSE}
model <- glm (target ~ ., data = crimeTRAIN, family = binomial(link="logit"))
model <- stepAIC(model, trace=FALSE, direction="both")
summary(model)
pred <- predict(model, newdata=subset(crimeTEST, select=c(1:12)), type='response')
cm <- confusionMatrix(as.factor(crimeTEST$target), as.factor(ifelse(pred > 0.5,1,0)))
cm$table
accuracy <- cm$overall['Accuracy']
R2 <- pR2(model)["McFadden"]
```

Model summary and confusion matrix of running this model against test data are above. The accuracy rate (`r accuracy`) is very good and the McFadden R^2 value (`r R2`) is also high. AIC value is 118.99. Additionally, consider the ROC curve for this model.

```{r echo=FALSE, warning=FALSE}
pr <- prediction(pred, crimeTEST$target)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf, colorize = TRUE, text.adj = c(-0.2,1.7))
auc <- performance(pr, measure = "auc")@y.values[[1]]
```

Area under the curve is `r auc`. 

The third model has the best (lowest) AIC value (better). Accuracy is the same as for the first model (and slightly lower than the second model). AUC is lower, but very close to the AUC value for the second model. Finally, McFadden R^2 falls between the first and second models, but the change is also very small. 

#### Additional Models

Basic models produced very efficient results. Several other models were attempted, but they did not produce significant improvements and therefore simplier, easier to interpret basic models were preferred. Other models included variable transformations and variable interactions. Since this project does not deals with critical and sensitive data with high cost of errors, such as medical or national security projects may, the accuracy of the basic models is deemed acceptable.

## Model Selection

All 3 models generated good overall results, but the third model (_StepAIC_ model) is chosen for its simplicity. It is important to note that even though general parameters between models are close, one may be preferred over the other based on application. For example, the second and third models have similar number of errors (6 and 7); however, the second model has more Type II/False Negative errors and less Type I/False Positive errors than the third model. This difference in sensitivity and specificity may be important for some applications. 

Additionally, one small adjustment to the model is to convert `nox` from parts per 10 million to parts per 100,000. This will help interpret the model coefficients. 

```{r echo=FALSE, warning=FALSE}
model <- glm(formula = target ~ zn + I(nox*100) + rad + tax, 
             family = binomial(link = "logit"), data = crime)
summary(model)
```

## Model Performance and Description

#### K-Fold Cross Validation

To assess the performance of selected model, below are results of 10-fold cross-validation. The model performs as expected.

```{r echo=FALSE, warning=FALSE}
ctrl <- trainControl(method = "repeatedcv", number = 10, savePredictions = TRUE)
model_fit <- train(target ~ zn + nox + rad + tax,  data=crimeTRAIN, 
                   method="glm", family="binomial",
                   trControl = ctrl, tuneLength = 5)
pred <- predict(model_fit, newdata=crimeTEST)
confusionMatrix(as.factor(crimeTEST$target), as.factor(ifelse(pred > 0.5,1,0)))
```

#### Deviance 

Similarly, the deviance table below demonstrated that each variable significantly contributes to the drop in deviance difference.

```{r echo=FALSE, warning=FALSE}
anova(model, test="Chisq")
```

#### Variance Inflation Factor

```{r echo=FALSE, warning=FALSE}
vif(model)
```

VIFs are reasonable, so that we can assume that there is not much multicollinearity between variables.

#### Histogram of Predicted Probabilities

Distribution of predicted probabilities generated by running all training data through the model shows that the model is predicting 0 or 1 with high probability. There are few instances where probability shows less certainty in the selected outcome.

```{r echo=FALSE, warning=FALSE}
pred <- predict(model, newdata = subset(crime, select=c(1:12)), type='response')
hist(pred, breaks=100, xlab="Predicted Probability of Outcome", main="")
```

#### Coefficients/Odds/Variable Contribution

```{r echo=FALSE, warning=FALSE}
as.data.frame(exp(model$coefficients))
```

For `zn`, the coefficient is negative and the odds of having an above median crime rate is 0.9154. Higher `zn`, meaning more large lots, is less likely to increase the crime rate. 

For `nox`, the coefficient is positive and the odds is 1.75, so there is a 75% increase in odds with higher `nox` values. Higher levels of nitrogen oxide indicate more congested neighborhoods. It is possible to theorize that more urban, congested areas are more likely to have higher crime than suburban areas.

For `tax`, the coefficient is negative and the odds is 0.984. Decrease in crime rate is more likely with the increase of property-tax rates. 

Finally, for `rad` all coefficients are positive except for `rad` value of 2. There is no explicit explanation for values of `rad` variable. Assuming that low values mean higher accessibility to radial highways, it is possible to theorize that living close, but not too close to highways is more likely to decrease the crime rate, but then moving away from highways is more likely to increase it. Odds are difficult to intepret possibly because of outliers (most likely `rad` value of 24).

## Evaluation Data Set

```{r echo=FALSE, warning=FALSE}
eval <- read.csv(url(paste0("https://raw.githubusercontent.com/omerozeren/DATA621/master/crime-evaluation-data_modified.csv")),
                na.strings=c("","NA"))
eval[,'rad'] <- as.factor(eval[,'rad'])
eval[,'chas'] <- as.factor(eval[,'chas'])
pred <- predict(model, newdata=eval, type="response")
eval <- cbind(eval, prob=round(pred,4))
eval <- cbind(eval, predict=round(pred,0))
kable(eval, row.names=FALSE)
```

Split between predicted outcomes is illustrated by tables below. 

```{r echo=FALSE, warning=FALSE}
table(eval$pred)
table(eval$pred)/sum(table(eval$pred))
``